"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4120],{4096:(e,t,n)=>{n.d(t,{A:()=>r});n(6540);var a=n(7377),i=n(4848);const r=e=>{let{conversation:t,userAvatar:n,chatbotAvatar:r}=e;const{colorMode:s}=(0,a.G)(),l=e=>{let t,n;return"dark"===s?(t="user"===e?"#333":"#1a4d57",n="#fff"):(t="user"===e?"#f0f0f0":"#d1e7dd",n="#000"),{backgroundColor:t,color:n,padding:"10px 15px",borderRadius:"15px",margin:"0 10px",maxWidth:"70%"}};return(0,i.jsx)("div",{style:{fontFamily:'"Helvetica Neue", Helvetica, Arial, sans-serif',maxWidth:"90%",margin:"auto",color:"dark"===s?"var(--ifm-heading-color)":"inherit"},children:t.map(((e,t)=>(0,i.jsxs)("div",{style:{display:"flex",alignItems:"flex-end",flexDirection:"user"===e.speaker?"row":"row-reverse",marginBottom:"10px"},children:[(0,i.jsx)("img",{src:"user"===e.speaker?n:r,alt:e.speaker,style:{width:"40px",height:"40px",borderRadius:"20px"}}),(0,i.jsx)("div",{style:l(e.speaker),children:(0,i.jsx)("p",{style:{margin:"0"},children:e.text})}),e.comment&&(0,i.jsx)("div",{style:{flex:1,padding:"0 10px",fontSize:"0.8em",color:"dark"===s?"#ccc":"#555",textAlign:"user"===e.speaker?"right":"left",alignSelf:"center"},children:e.comment})]},t)))})}},7126:(e,t,n)=>{n.d(t,{A:()=>s});var a=n(6540),i=n(2214),r=n(4848);function s(e){let{image:t,alt:s,caption:l}=e;const[o,d]=(0,a.useState)({width:0,height:0}),c=l.split("\\n").map(((e,t,n)=>{const i=e.replace(/\[([^\]]+)\]\((https?:\/\/[^\s]+)\)/g,((e,t,n)=>`<a href="${n}" target="_blank" rel="noopener noreferrer">${t}</a>`));return(0,r.jsxs)(a.Fragment,{children:[(0,r.jsx)("span",{dangerouslySetInnerHTML:{__html:i}}),t<n.length-1&&(0,r.jsx)("br",{})]},t)}));return(0,a.useEffect)((()=>{const e=new Image;e.onload=()=>{d({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const a=new i.A({gallery:"#figure-gallery",children:"a",pswpModule:()=>n.e(8300).then(n.bind(n,8300))});a.init();const r=document.querySelectorAll("#figure-gallery figcaption a");return r.forEach((e=>{e.addEventListener("click",(e=>{e.stopPropagation()}))})),()=>{a.destroy(),r.forEach((e=>{e.removeEventListener("click",(e=>{e.stopPropagation()}))}))}}),[t]),(0,r.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,r.jsx)("a",{href:t,"data-pswp-width":o.width,"data-pswp-height":o.height,children:(0,r.jsx)("img",{src:t,alt:s,style:{maxWidth:"100%",height:"auto"}})}),(0,r.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,r.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:c})]})}},4881:(e,t,n)=>{n.d(t,{A:()=>r});var a=n(6540),i=n(4848);function r(e){let{videoSrc:t,alt:n,caption:r,autoPlay:s=!1}=e;const[l,o]=(0,a.useState)({width:0,height:0}),d=(0,a.useRef)(null),[c,g]=(0,a.useState)(!1),h=r.split("\\n").map(((e,t,n)=>{const r=e.replace(/\[([^\]]+)\]\((https?:\/\/[^\s]+)\)/g,((e,t,n)=>`<a href="${n}" target="_blank" rel="noopener noreferrer">${t}</a>`));return(0,i.jsxs)(a.Fragment,{children:[(0,i.jsx)("span",{dangerouslySetInnerHTML:{__html:r}}),t<n.length-1&&(0,i.jsx)("br",{})]},t)}));return(0,a.useEffect)((()=>{g("ontouchstart"in window)}),[]),(0,a.useEffect)((()=>{const e=document.createElement("video");e.onloadedmetadata=()=>{o({width:e.videoWidth,height:e.videoHeight})},e.src=t}),[t]),(0,a.useEffect)((()=>{const e=d.current;e&&s&&!c&&e.play().catch((e=>console.error("Autoplay failed:",e)))}),[s,c]),(0,i.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},children:[(0,i.jsx)("video",{ref:d,src:t,alt:n,controls:!0,loop:!0,playsInline:!0,muted:!c,style:{maxWidth:"100%",height:"auto"}}),(0,i.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,i.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:h})]})}},4600:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>u,frontMatter:()=>o,metadata:()=>a,toc:()=>g});var a=n(4415),i=n(4848),r=n(8453),s=(n(3320),n(7126)),l=(n(4881),n(4096),n(4706));n(838),n(482),n(4567),n(8273),n(1769),n(9063),n(1490);const o={slug:"inference-engines",title:"Open Source Inference at Full Throttle: Exploring TGI and vLLM",authors:["ikelk"],tags:["LLM","ChatGPT","AI","vLLM","PagedAttention","LocalAI","OpenWeightsModels","LlamaModels","InferenceEngine","KVCache"],enableComments:!0,image:"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true",hide_reading_time:!1},d=void 0,c={authorsImageUrls:[void 0]},g=[];function h(e){const t={admonition:"admonition",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI\u2019s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn\u2019t be changed. While this was necessary due to the model's enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required."}),"\n",(0,i.jsx)(t.p,{children:"Two years later and the situation has dramatically changed. Due to the rising availability of open-weights alternatives like Meta\u2019s Llama models, we now have multiple options for running LLMs locally on our own hardware. Access is no longer tethered to cloud-based infrastructures."}),"\n",(0,i.jsx)(s.A,{image:l.A,alt:"Two old-timey F1 race cars labeled TGI and vLLM, capturing that vintage racing vibe with a touch of futuristic flair. This design emphasizes the competitive spirit between the two inference engines, set in a nostalgic, dynamic scene.",caption:"Generated with OpenAI DALL-E 3 and edited by the author."}),"\n",(0,i.jsx)(t.admonition,{title:"Some key points I'll address here are:",type:"tip",children:(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"The transition from server-based LLMs like ChatGPT to locally runnable models, enabling customization and offline usage."}),"\n",(0,i.jsx)(t.li,{children:"The role of inference engines in executing neural networks using learned parameters for local model inference."}),"\n",(0,i.jsxs)(t.li,{children:["Introduction to ",(0,i.jsx)(t.strong,{children:"PagedAttention"})," in vLLM, improving memory efficiency through better key-value cache management."]}),"\n",(0,i.jsx)(t.li,{children:"A comparison of TGI and vLLM, highlighting shared features such as tensor parallelism and batching, and distinct features like speculative decoding and structured output guidance."}),"\n",(0,i.jsxs)(t.li,{children:["Explanation of ",(0,i.jsx)(t.strong,{children:"latency"})," and ",(0,i.jsx)(t.strong,{children:"throughput"}),", including how these performance metrics influence LLM deployments."]}),"\n",(0,i.jsx)(t.li,{children:"Advice on selecting between TGI and vLLM based on specific enterprise needs, focusing on use case experimentation and benchmarking."}),"\n",(0,i.jsxs)(t.li,{children:["An overview of licensing differences, discussing TGI's shift back to ",(0,i.jsx)(t.strong,{children:"Apache 2.0"}),", aligning with vLLM\u2019s license."]}),"\n",(0,i.jsx)(t.li,{children:"Practical code examples showing how to deploy models using both TGI and vLLM for real-world applications."}),"\n"]})})]})}function u(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},838:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/andrej-aa31302b343b5abbbf647be2931fea67.jpg"},482:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/chatgpt-photosynthesis-2-f3fb095ef8519a0ef8b7681e7719670c.jpeg"},4567:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/comparison-1f235c6948edbb826bea190711cfcbe1.jpg"},1769:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/paged-b531baf91d73108133090f7951f8a25d.jpg"},8273:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/sequential-df783d324f8d6a36f42b1acffd4c76de.jpg"},9063:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/shrug-a2ee30b27da5c96b3201ad1b30b37974.jpg"},4706:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/social-card-ac5de118d9a61de15beb773dc746b98b.jpg"},1490:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/medias/LatencyThroughputVisualization-d3416c070c2f04dfca64adc07fe280b4.webm"},4415:e=>{e.exports=JSON.parse('{"permalink":"/blog/inference-engines","source":"@site/blog/2024-09-28-inference-engines/index.md","title":"Open Source Inference at Full Throttle: Exploring TGI and vLLM","description":"Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI\u2019s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn\u2019t be changed. While this was necessary due to the model\'s enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required.","date":"2024-09-28T00:00:00.000Z","tags":[{"inline":true,"label":"LLM","permalink":"/blog/tags/llm"},{"inline":true,"label":"ChatGPT","permalink":"/blog/tags/chat-gpt"},{"inline":true,"label":"AI","permalink":"/blog/tags/ai"},{"inline":true,"label":"vLLM","permalink":"/blog/tags/v-llm"},{"inline":true,"label":"PagedAttention","permalink":"/blog/tags/paged-attention"},{"inline":true,"label":"LocalAI","permalink":"/blog/tags/local-ai"},{"inline":true,"label":"OpenWeightsModels","permalink":"/blog/tags/open-weights-models"},{"inline":true,"label":"LlamaModels","permalink":"/blog/tags/llama-models"},{"inline":true,"label":"InferenceEngine","permalink":"/blog/tags/inference-engine"},{"inline":true,"label":"KVCache","permalink":"/blog/tags/kv-cache"}],"readingTime":10.257894736842106,"hasTruncateMarker":true,"authors":[{"name":"Ian Kelk","title":"Developer Relations","url":"https://kelk.ai","socials":{"linkedin":"https://www.linkedin.com/in/iankelk/","github":"https://github.com/iankelk"},"imageURL":"https://github.com/iankelk.png","key":"ikelk","page":null}],"frontMatter":{"slug":"inference-engines","title":"Open Source Inference at Full Throttle: Exploring TGI and vLLM","authors":["ikelk"],"tags":["LLM","ChatGPT","AI","vLLM","PagedAttention","LocalAI","OpenWeightsModels","LlamaModels","InferenceEngine","KVCache"],"enableComments":true,"image":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true","hide_reading_time":false},"unlisted":false,"nextItem":{"title":"Secret LLM chickens II: Tuning the chicken","permalink":"/blog/secret-chickens-tuning"}}')}}]);