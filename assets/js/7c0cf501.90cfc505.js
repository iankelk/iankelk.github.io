"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4120],{2822:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>d,toc:()=>g});var a=n(4848),i=n(8453),l=(n(1432),n(7242)),r=(n(7476),n(9288));n(4352),n(6240),n(5357),n(4887),n(23);const s={slug:"inference-engines",title:"Open Source Inference at Full Throttle: Exploring TGI and vLLM",authors:["ikelk"],tags:["LLM","ChatGPT","AI","vLLM","PagedAttention","LocalAI","OpenWeightsModels","LlamaModels","InferenceEngine","KVCache"],enableComments:!0,image:"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true",hide_reading_time:!1},o=void 0,d={permalink:"/blog/inference-engines",source:"@site/blog/2024-09-28-inference-engines/index.md",title:"Open Source Inference at Full Throttle: Exploring TGI and vLLM",description:"Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI\u2019s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn\u2019t be changed. While this was necessary due to the model's enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required.",date:"2024-09-28T00:00:00.000Z",tags:[{inline:!0,label:"LLM",permalink:"/blog/tags/llm"},{inline:!0,label:"ChatGPT",permalink:"/blog/tags/chat-gpt"},{inline:!0,label:"AI",permalink:"/blog/tags/ai"},{inline:!0,label:"vLLM",permalink:"/blog/tags/v-llm"},{inline:!0,label:"PagedAttention",permalink:"/blog/tags/paged-attention"},{inline:!0,label:"LocalAI",permalink:"/blog/tags/local-ai"},{inline:!0,label:"OpenWeightsModels",permalink:"/blog/tags/open-weights-models"},{inline:!0,label:"LlamaModels",permalink:"/blog/tags/llama-models"},{inline:!0,label:"InferenceEngine",permalink:"/blog/tags/inference-engine"},{inline:!0,label:"KVCache",permalink:"/blog/tags/kv-cache"}],readingTime:6.544736842105263,hasTruncateMarker:!0,authors:[{name:"Ian Kelk",title:"Developer Relations @ Hume AI",url:"https://kelk.ai",imageURL:"https://github.com/iankelk.png",key:"ikelk"}],frontMatter:{slug:"inference-engines",title:"Open Source Inference at Full Throttle: Exploring TGI and vLLM",authors:["ikelk"],tags:["LLM","ChatGPT","AI","vLLM","PagedAttention","LocalAI","OpenWeightsModels","LlamaModels","InferenceEngine","KVCache"],enableComments:!0,image:"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true",hide_reading_time:!1},unlisted:!1,nextItem:{title:"Secret LLM chickens II: Tuning the chicken",permalink:"/blog/secret-chickens-tuning"}},c={authorsImageUrls:[void 0]},g=[];function h(e){const t={admonition:"admonition",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI\u2019s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn\u2019t be changed. While this was necessary due to the model's enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required."}),"\n",(0,a.jsx)(t.p,{children:"Two years later and the situation has dramatically changed. Thanks to the rise of open-weights alternatives like Meta\u2019s Llama models, we now have multiple options for running LLMs locally on our own hardware. Access is no longer tethered to cloud-based infrastructures, but instead users can directly manipulate, explore, and deploy models themselves."}),"\n",(0,a.jsx)(l.A,{image:r.A,alt:"Two old-timey F1 race cars labeled TGI and vLLM, capturing that vintage racing vibe with a touch of futuristic flair. This design emphasizes the competitive spirit between the two inference engines, set in a nostalgic, dynamic scene.",caption:"Generated with OpenAI DALL-E 3 and edited by the author."}),"\n",(0,a.jsx)(t.admonition,{title:"Some key points I'll address here are:",type:"tip",children:(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"The evolution from server-dependent AI models like ChatGPT to locally runnable models like Meta's Llama"}),"\n",(0,a.jsx)(t.li,{children:"The basic components needed to run a local LLM: parameter files and inference engine code"}),"\n",(0,a.jsx)(t.li,{children:"The introduction of PagedAttention and its role in efficient memory management for LLM serving"}),"\n",(0,a.jsx)(t.li,{children:"The comparison between traditional contiguous memory allocation and the paged memory approach in AI models"}),"\n",(0,a.jsx)(t.li,{children:"The ongoing competition and rapid developments in LLM inference optimization, including vLLM and TGI"}),"\n"]})})]})}function m(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},7476:(e,t,n)=>{n.d(t,{A:()=>l});n(6540);var a=n(5293),i=n(4848);const l=e=>{let{conversation:t,userAvatar:n,chatbotAvatar:l}=e;const{colorMode:r}=(0,a.G)(),s=e=>{let t,n;return"dark"===r?(t="user"===e?"#333":"#1a4d57",n="#fff"):(t="user"===e?"#f0f0f0":"#d1e7dd",n="#000"),{backgroundColor:t,color:n,padding:"10px 15px",borderRadius:"15px",margin:"0 10px",maxWidth:"70%"}};return(0,i.jsx)("div",{style:{fontFamily:'"Helvetica Neue", Helvetica, Arial, sans-serif',maxWidth:"90%",margin:"auto",color:"dark"===r?"var(--ifm-heading-color)":"inherit"},children:t.map(((e,t)=>(0,i.jsxs)("div",{style:{display:"flex",alignItems:"flex-end",flexDirection:"user"===e.speaker?"row":"row-reverse",marginBottom:"10px"},children:[(0,i.jsx)("img",{src:"user"===e.speaker?n:l,alt:e.speaker,style:{width:"40px",height:"40px",borderRadius:"20px"}}),(0,i.jsx)("div",{style:s(e.speaker),children:(0,i.jsx)("p",{style:{margin:"0"},children:e.text})}),e.comment&&(0,i.jsx)("div",{style:{flex:1,padding:"0 10px",fontSize:"0.8em",color:"dark"===r?"#ccc":"#555",textAlign:"user"===e.speaker?"right":"left",alignSelf:"center"},children:e.comment})]},t)))})}},7242:(e,t,n)=>{n.d(t,{A:()=>r});var a=n(6540),i=n(2214),l=n(4848);function r(e){let{image:t,alt:r,caption:s}=e;const[o,d]=(0,a.useState)({width:0,height:0}),c=s.split("\\n").map(((e,t,n)=>{const i=e.replace(/\[([^\]]+)\]\((https?:\/\/[^\s]+)\)/g,((e,t,n)=>`<a href="${n}" target="_blank" rel="noopener noreferrer">${t}</a>`));return(0,l.jsxs)(a.Fragment,{children:[(0,l.jsx)("span",{dangerouslySetInnerHTML:{__html:i}}),t<n.length-1&&(0,l.jsx)("br",{})]},t)}));return(0,a.useEffect)((()=>{const e=new Image;e.onload=()=>{d({width:e.naturalWidth,height:e.naturalHeight})},e.src=t;const a=new i.A({gallery:"#figure-gallery",children:"a",pswpModule:()=>n.e(8300).then(n.bind(n,8300))});a.init();const l=document.querySelectorAll("#figure-gallery figcaption a");return l.forEach((e=>{e.addEventListener("click",(e=>{e.stopPropagation()}))})),()=>{a.destroy(),l.forEach((e=>{e.removeEventListener("click",(e=>{e.stopPropagation()}))}))}}),[t]),(0,l.jsxs)("figure",{style:{border:"1px dashed rgba(0, 0, 0, .1)",padding:0,margin:0,marginBottom:20,borderRadius:"15px",textAlign:"right"},id:"figure-gallery",children:[(0,l.jsx)("a",{href:t,"data-pswp-width":o.width,"data-pswp-height":o.height,children:(0,l.jsx)("img",{src:t,alt:r,style:{maxWidth:"100%",height:"auto"}})}),(0,l.jsx)("hr",{style:{margin:"5px 0",backgroundColor:"rgba(0, 0, 0, .2)"}}),(0,l.jsx)("figcaption",{style:{marginTop:"0.5em",marginBottom:"0.5em",marginRight:"1em",textAlign:"right",fontSize:"0.8em"},children:c})]})}},4352:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/andrej-aa31302b343b5abbbf647be2931fea67.jpg"},6240:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/chatgpt-photosynthesis-2-f3fb095ef8519a0ef8b7681e7719670c.jpeg"},5357:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/comparison-1f235c6948edbb826bea190711cfcbe1.jpg"},23:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/paged-b531baf91d73108133090f7951f8a25d.jpg"},4887:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/sequential-df783d324f8d6a36f42b1acffd4c76de.jpg"},9288:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/social-card-ac5de118d9a61de15beb773dc746b98b.jpg"}}]);