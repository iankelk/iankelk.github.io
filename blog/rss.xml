<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Short Attention Blog</title>
        <link>https://iankelk.github.io/blog</link>
        <description>Short Attention Blog</description>
        <lastBuildDate>Sun, 26 Nov 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How ChatGPT fools us into thinking we're having a conversation]]></title>
            <link>https://iankelk.github.io/blog/how-chatgpt-fools-us</link>
            <guid>https://iankelk.github.io/blog/how-chatgpt-fools-us</guid>
            <pubDate>Sun, 26 Nov 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Remember the first time you used ChatGPT and how amazed you were to find yourself having what appeared to be a full-on conversation with an artificial intelligence? While ChatGPT was (and still is) mind-blowing, it uses a few tricks to make things appear more familiar.]]></description>
            <content:encoded><![CDATA[<p>Remember the first time you used ChatGPT and how amazed you were to find yourself having what appeared to be a full-on conversation with an artificial intelligence? While ChatGPT was (and still is) mind-blowing, it uses a few tricks to make things appear more familiar.</p>
<p><img loading="lazy" alt="ChatGPT on the Tonight Show" src="https://iankelk.github.io/assets/images/social-card-e9b8326af5cd7ed2f1080c7e0dcc443c.jpg" width="1200" height="675" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="trick-1-every-time-you-talk-to-chatgpt-youre-not-just-sending-it-your-question-youre-also-sending-it-the-entire-conversation-up-until-that-point">Trick #1: Every time you talk to ChatGPT, you're not just sending it your question. You're also sending it the <em>entire</em> conversation up until that point.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#trick-1-every-time-you-talk-to-chatgpt-youre-not-just-sending-it-your-question-youre-also-sending-it-the-entire-conversation-up-until-that-point" class="hash-link" aria-label="Direct link to trick-1-every-time-you-talk-to-chatgpt-youre-not-just-sending-it-your-question-youre-also-sending-it-the-entire-conversation-up-until-that-point" title="Direct link to trick-1-every-time-you-talk-to-chatgpt-youre-not-just-sending-it-your-question-youre-also-sending-it-the-entire-conversation-up-until-that-point">​</a></h2>
<p>Contrary to appearances, large language models (LLMs) like ChatGPT do not actually "remember" past interactions. The moment they finish "typing" out their response, they have no idea who you are or what you were talking about. When ChatGPT seems to naturally recall details from earlier in the conversation, it is an illusion; the context of that dialogue is given back to ChatGPT every time you say something to it. This context enables them to build coherent, follow-on responses that appear to be normal conversations.</p>
<p>However, without this context, ChatGPT would have no knowledge of what was previously discussed. Like all LLMs, ChatGPT is completely <em>stateless</em>, meaning that in the actual model itself, no information is maintained between inputs and outputs. All of this feeding of previous context into the current interaction is hidden behind the scenes in the ChatGPT web application.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-typical-short-conversation-with-chatgpt-might-go-like-this">A typical short conversation with ChatGPT might go like this.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#a-typical-short-conversation-with-chatgpt-might-go-like-this" class="hash-link" aria-label="Direct link to A typical short conversation with ChatGPT might go like this." title="Direct link to A typical short conversation with ChatGPT might go like this.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-photosynthesis-1-c2aa8fee81245c48d51d92df0afc0aa5.jpeg" width="1320" height="1566" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="however-this-is-what-is-actually-going-on-behind-the-scenes">However, this is what is actually going on behind the scenes.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#however-this-is-what-is-actually-going-on-behind-the-scenes" class="hash-link" aria-label="Direct link to However, this is what is actually going on behind the scenes." title="Direct link to However, this is what is actually going on behind the scenes.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-photosynthesis-2-f3fb095ef8519a0ef8b7681e7719670c.jpeg" width="1320" height="1561" class="img_ev3q"></p>
<p>Notice that when the woman asks her second question, she has to reiterate the entire previous conversation, complete with tags on who said what. Can you imagine talking to a person where every time it was your turn to speak, you had to repeat the entire conversation up to that point? This is how ChatGPT (and all current LLMs) work. They require using their own outputs, plus the prompts that generated these outputs, to be prepended to the start of every new prompt from the user.</p>
<p>These models are termed "auto-regressive" due to their method of generating text one piece at a time, building upon the previously generated text. "Auto-" comes from the Greek word "autós," meaning "self," and "regressive" is derived from "regress," which in this context refers to the statistical method of predicting future values based on past values.</p>
<p>In LLMs, what this means is that the model predicts the next word or token in a sequence based on <em>all</em> the words or tokens that have come before it. That's <em>all</em> of it, not just the current question being asked in a long back-and-forth chat conversation. In humans, we naturally maintain coherence and context in a conversation by just... participating in the conversation.</p>
<p>However, while chats with ChatGPT mimic a conversational style, with each response building upon the previous dialogue, the moment ChatGPT finishes writing a response, it has no memory of what it just said. Take a look at what would happen with this same conversation without the <em>entire discourse</em> being back fed to ChatGPT behind the scenes:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-chatgpt-would-respond-without-being-fed-the-whole-conversation">How ChatGPT would respond without being fed the whole conversation.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#how-chatgpt-would-respond-without-being-fed-the-whole-conversation" class="hash-link" aria-label="Direct link to How ChatGPT would respond without being fed the whole conversation." title="Direct link to How ChatGPT would respond without being fed the whole conversation.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT would respond without being fed the whole conversation:" src="https://iankelk.github.io/assets/images/chatgpt-photosynthesis-3-12c4e20361df1dbcb1838a483460eb32.jpeg" width="1320" height="1563" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="trick-2-chatgpt-doesnt-forget-things-over-time-but-over-the-length-of-the-conversation">Trick #2: ChatGPT doesn't forget things over time, but over the length of the conversation<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#trick-2-chatgpt-doesnt-forget-things-over-time-but-over-the-length-of-the-conversation" class="hash-link" aria-label="Direct link to Trick #2: ChatGPT doesn't forget things over time, but over the length of the conversation" title="Direct link to Trick #2: ChatGPT doesn't forget things over time, but over the length of the conversation">​</a></h2>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="context-length-in-llms">Context Length in LLMs<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#context-length-in-llms" class="hash-link" aria-label="Direct link to Context Length in LLMs" title="Direct link to Context Length in LLMs">​</a></h4>
<p>When ChatGPT first came out in November 2022, it only offered the model GPT-3.5, which had a maximum context of 4,096 tokens, which is roughly 3,000 words.</p>
<p>This means that the model can comprehend a maximum of 4,096 tokens at any point. Tokenization is a fascinating subject in itself, and my next post will cover how it works and why 4,096 tokens only gives you about 3,000 words.</p>
<p>There has been some confusion about what the token limit means: can we give ChatGPT 3,000 words and expect it to be able to produce 3,000 words back? The answer is unfortunately no; the context length of 4,096 tokens covers both the input (prompt) and the output (response). This results in a trade-off where we have to balance the amount of information we give in a prompt with the length of the response we get from the model.</p>
<ol>
<li>
<p><em>Input (Prompt):</em> A longer prompt leaves less room for a meaningful response; if the input uses 4,000 tokens, the response can only be 96 tokens long to stay within the token limit.</p>
</li>
<li>
<p><em>Output (Response):</em> A shorter prompt could lead to a longer response as long as the combined length doesn't exceed the token limit, but you may not be able to include information in the prompt.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-chat-problem">The chat problem<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#the-chat-problem" class="hash-link" aria-label="Direct link to The chat problem" title="Direct link to The chat problem">​</a></h3>
<p>Do you see where this becomes problematic? In the first part of this post, I explain how the entire conversation has to be fed to the model so that it remembers what has already been discussed. Combining this with the context length, the result is that as you talk more and more with ChatGPT, eventually the combined totals of what you've asked and what it has replied will exceed the 4,096 token limit, and it won't be able to answer any more.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-visualization-of-chatgpt-simultaneously-printing-and-scanning-back-in-the-entire-conversation-as-it-grows-to-extreme-proportions">A visualization of ChatGPT simultaneously printing and scanning back in the entire conversation as it grows to extreme proportions.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#a-visualization-of-chatgpt-simultaneously-printing-and-scanning-back-in-the-entire-conversation-as-it-grows-to-extreme-proportions" class="hash-link" aria-label="Direct link to A visualization of ChatGPT simultaneously printing and scanning back in the entire conversation as it grows to extreme proportions." title="Direct link to A visualization of ChatGPT simultaneously printing and scanning back in the entire conversation as it grows to extreme proportions.">​</a></h3>
<p><img loading="lazy" alt="ChatGPT accumulating a very long chat" src="https://iankelk.github.io/assets/images/rolled-chatgpt-51756537e64b16efdf83b5001c773e26.jpg" width="1024" height="1024" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="conversation-length-and-token-limitations-in-llms">Conversation Length and Token Limitations in LLMs<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#conversation-length-and-token-limitations-in-llms" class="hash-link" aria-label="Direct link to Conversation Length and Token Limitations in LLMs" title="Direct link to Conversation Length and Token Limitations in LLMs">​</a></h4>
<p>As a conversation between a human and an LLM chatbot grows, they run the risk of exceeding the model's context window (e.g., 4,096 tokens for GPT-3.5). This is handled by the web version of ChatGPT, which invisibly removes the oldest parts of the conversation to remain within the limit. This method—using a rolling window of context—is certainly one of the easiest to implement.</p>
<p>What does this mean? The longer your conversation, the sooner ChatGPT will start forgetting things you said at the beginning of the chat.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="another-visualization-of-chatgpt-trimming-away-the-start-of-your-conversation-behind-the-scenes">Another visualization of ChatGPT trimming away the start of your conversation behind the scenes.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#another-visualization-of-chatgpt-trimming-away-the-start-of-your-conversation-behind-the-scenes" class="hash-link" aria-label="Direct link to Another visualization of ChatGPT trimming away the start of your conversation behind the scenes." title="Direct link to Another visualization of ChatGPT trimming away the start of your conversation behind the scenes.">​</a></h3>
<p><img loading="lazy" alt="ChatGPT accumulating a very long chat" src="https://iankelk.github.io/assets/images/chatgpt-cutting-a46d0da8fb8e542b1fc4668473f7d58b.jpg" width="1024" height="1024" class="img_ev3q"></p>
<p>It's good to be mindful of this restriction, especially when referring back to earlier parts of a conversation that might have been truncated due to token limitations—the LLM will not be able to recall these anymore, but the web version of ChatGPT will not tell you. There's also always the risk that it could hallucinate answers based on other parts of the conversation if the beginning is trimmed off.</p>
<p>Let's take another look at what happens in a more complex yet sillier chat interaction.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="another-typical-but-silly-conversation-with-chatgpt">Another typical but silly conversation with ChatGPT.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#another-typical-but-silly-conversation-with-chatgpt" class="hash-link" aria-label="Direct link to Another typical but silly conversation with ChatGPT." title="Direct link to Another typical but silly conversation with ChatGPT.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-name-1-dc45db4079f1e8dafba7ab49b90ae76b.jpeg" width="2550" height="2353" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="again-this-is-what-is-actually-going-on-behind-the-scenes">Again, this is what is actually going on behind the scenes.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#again-this-is-what-is-actually-going-on-behind-the-scenes" class="hash-link" aria-label="Direct link to Again, this is what is actually going on behind the scenes." title="Direct link to Again, this is what is actually going on behind the scenes.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-name-2-67272e5821cb56c4a9ca8fd3edd0dd51.jpeg" width="2550" height="2365" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="now-lets-suppose-we-have-a-long-enough-conversation-that-the-beginning-is-trimmed-off-chatgpt-might-either-state-that-its-forgotten-the-name-entirely-or-hallucinate-it">Now let's suppose we have a long enough conversation that the beginning is trimmed off. ChatGPT might either state that it's forgotten the name entirely or hallucinate it.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#now-lets-suppose-we-have-a-long-enough-conversation-that-the-beginning-is-trimmed-off-chatgpt-might-either-state-that-its-forgotten-the-name-entirely-or-hallucinate-it" class="hash-link" aria-label="Direct link to Now let's suppose we have a long enough conversation that the beginning is trimmed off. ChatGPT might either state that it's forgotten the name entirely or hallucinate it." title="Direct link to Now let's suppose we have a long enough conversation that the beginning is trimmed off. ChatGPT might either state that it's forgotten the name entirely or hallucinate it.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-name-4-8ba86b96f0761141eb93a27cbb4d0e12.jpeg" width="2550" height="2463" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="and-finally-if-chatgpt-isnt-fed-the-entire-conversation-it-will-forget-it-all-the-moment-it-finishes-generating-each-answer">And finally, if ChatGPT isn't fed the entire conversation, it will forget it all the moment it finishes generating each answer.<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#and-finally-if-chatgpt-isnt-fed-the-entire-conversation-it-will-forget-it-all-the-moment-it-finishes-generating-each-answer" class="hash-link" aria-label="Direct link to And finally, if ChatGPT isn't fed the entire conversation, it will forget it all the moment it finishes generating each answer." title="Direct link to And finally, if ChatGPT isn't fed the entire conversation, it will forget it all the moment it finishes generating each answer.">​</a></h3>
<p><img loading="lazy" alt="How ChatGPT pretends to work" src="https://iankelk.github.io/assets/images/chatgpt-name-3-338403f314511d72e5f115cdd83ccdbf.jpeg" width="2550" height="2392" class="img_ev3q"></p>
<p>Since ChatGPT's debut in November 2022, GPT-4 has been released with both 8,192 and 32,768 context lengths. This made things a lot better in terms of tracking long conversations, and in November 2023, GPT-4 Turbo was released with a 128k context length. Things are looking increasingly good for these models' ability to track long conversations. However, despite GPT-4 Turbo's massive amount of context, it still has a completion limit of 4,096 tokens, so it will always generate a maximum of about 3,000 words.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-it-like-this">Why is it like this?<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#why-is-it-like-this" class="hash-link" aria-label="Direct link to Why is it like this?" title="Direct link to Why is it like this?">​</a></h2>
<p>Transformer-based models like GPT-3 and GPT-4 are designed to be stateless, for good reason! Primarily, this stateless nature significantly enhances scalability and efficiency. Each user request is processed independently, allowing the system to handle numerous queries simultaneously without the complexity of tracking ongoing conversations. Imagine the complexity if every time the model was called, it had to maintain some internal state across millions of users.</p>
<p>Transformer model hidden states are also temporary and exist only for the duration of processing a specific input sequence. Once the model has processed an input and generated an output, these states are reset. They do not persist between different interactions.</p>
<p>Data privacy and security play a role as well. Stateless models do not retain a memory of past interactions, ensuring that sensitive data from one session is never inadvertently exposed to another user. This design choice is particularly relevant in light of incidents like <a href="https://en.wikipedia.org/wiki/Tay_(chatbot)" target="_blank" rel="noopener noreferrer">Microsoft's Tay,</a> an AI chatbot that, due to its design to learn from interactions, ended up mimicking inappropriate and offensive language from users. It's just not safe to have models learn from inputs given by random users.</p>
<p>However, the stateless nature also means these models cannot remember user preferences or learn from past interactions. This is a limitation in scenarios where you might want to create personalized chatbots through past chats or systems that benefit from cumulative learning. To mitigate this, some implementations incorporate a stateful layer atop the stateless LLM, enabling personalized and continuous user experiences.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-a-bit-more-technical">Getting a bit more technical<a href="https://iankelk.github.io/blog/how-chatgpt-fools-us#getting-a-bit-more-technical" class="hash-link" aria-label="Direct link to Getting a bit more technical" title="Direct link to Getting a bit more technical">​</a></h2>
<p>Prominent examples of such layers include <a href="https://www.langchain.com/" target="_blank" rel="noopener noreferrer">LangChain</a>, <a href="https://www.llamaindex.ai/" target="_blank" rel="noopener noreferrer">LlamaIndex</a>, and <a href="https://haystack.deepset.ai/" target="_blank" rel="noopener noreferrer">Haystack</a>. These layers add flexibility in managing the limited context that LLMs can handle by offering various strategies. For instance, when approaching the token limit, choices must be made: Should a rolling window approach be used to discard older text like in the web ChatGPT, or should GPT be utilized to summarize previous information? Is it more important to retain the initial context, like a source article, while removing less critical middle or later sections? Alternatively, should retrieval augmented generation (RAG, more on that in a later blog) techniques be employed to integrate external data into the token stream? These decisions vary based on the specific goals of the implementation. The most effective architectures often consist of specialized components interwoven to achieve a wide array of practical outcomes, allowing for more nuanced and effective user interactions.</p>
<p><img loading="lazy" alt="A confused stateless robot" src="https://iankelk.github.io/assets/images/confused-1597cabfce805e83f79ef2cf41a2807c.jpg" width="1024" height="1024" class="img_ev3q"></p>]]></content:encoded>
            <category>history</category>
            <category>chatgpt</category>
            <category>context</category>
            <category>chat</category>
            <category>AI</category>
            <category>LLM</category>
            <category>chatbots</category>
            <category>AIExplained</category>
        </item>
    </channel>
</rss>