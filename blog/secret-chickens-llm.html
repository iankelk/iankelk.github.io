<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">The secret chickens that run LLMs | Short Attention</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kelk.ai/blog/secret-chickens-llm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="The secret chickens that run LLMs | Short Attention"><meta data-rh="true" name="description" content="Humans often organize large, skilled groups to undertake complex projects and then bizarrely place incompetent people in charge. Large language models (LLMs) such as OpenAI GPT-4, Anthropic Claude, and Google Gemini carry on this proud tradition with my new favorite metaphor of who has the final say in writing the text they generate—a chicken."><meta data-rh="true" property="og:description" content="Humans often organize large, skilled groups to undertake complex projects and then bizarrely place incompetent people in charge. Large language models (LLMs) such as OpenAI GPT-4, Anthropic Claude, and Google Gemini carry on this proud tradition with my new favorite metaphor of who has the final say in writing the text they generate—a chicken."><meta data-rh="true" property="og:image" content="https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-05-06-chickens/social-card.jpg?raw=true"><meta data-rh="true" name="twitter:image" content="https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-05-06-chickens/social-card.jpg?raw=true"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-05-06T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://kelk.ai"><meta data-rh="true" property="article:tag" content="distribution,sampling,AGI,ASI,chatgpt,chat,AI,LLM,ML,chatbot,chatbots,stochastic chicken,stochastic parrot,AIExplained"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kelk.ai/blog/secret-chickens-llm"><link data-rh="true" rel="alternate" href="https://kelk.ai/blog/secret-chickens-llm" hreflang="en"><link data-rh="true" rel="alternate" href="https://kelk.ai/blog/secret-chickens-llm" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://kelk.ai/blog/secret-chickens-llm","mainEntityOfPage":"https://kelk.ai/blog/secret-chickens-llm","url":"https://kelk.ai/blog/secret-chickens-llm","headline":"The secret chickens that run LLMs","name":"The secret chickens that run LLMs","description":"Humans often organize large, skilled groups to undertake complex projects and then bizarrely place incompetent people in charge. Large language models (LLMs) such as OpenAI GPT-4, Anthropic Claude, and Google Gemini carry on this proud tradition with my new favorite metaphor of who has the final say in writing the text they generate—a chicken.","datePublished":"2024-05-06T00:00:00.000Z","author":{"@type":"Person","name":"Ian Kelk","description":"Developer Relations @ Hume AI","url":"https://kelk.ai","image":"https://github.com/iankelk.png"},"image":{"@type":"ImageObject","@id":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-05-06-chickens/social-card.jpg?raw=true","url":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-05-06-chickens/social-card.jpg?raw=true","contentUrl":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-05-06-chickens/social-card.jpg?raw=true","caption":"title image for the blog post: The secret chickens that run LLMs"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://kelk.ai/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Short Attention RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Short Attention Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Short Attention JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PNHB98RZ0D"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PNHB98RZ0D",{anonymize_ip:!0})</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.049cba7a.css">
<script src="/assets/js/runtime~main.93b37b89.js" defer="defer"></script>
<script src="/assets/js/main.a97e7c1a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Short Attention Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Short Attention Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Projects Page</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Short Attention Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/iankelk" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/secret-chickens-tuning">Secret LLM chickens II: Tuning the chicken</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/secret-chickens-llm">The secret chickens that run LLMs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-forward-thinking">LLMs are forward thinkers, and that&#x27;s a bit of a problem</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/how-chatgpt-fools-us">How ChatGPT fools us into thinking we&#x27;re having a conversation</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">The secret chickens that run LLMs</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-05-06T00:00:00.000Z">May 6, 2024</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://kelk.ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/iankelk.png" alt="Ian Kelk"></a><div class="avatar__intro"><div class="avatar__name"><a href="https://kelk.ai" target="_blank" rel="noopener noreferrer"><span>Ian Kelk</span></a></div><small class="avatar__subtitle">Developer Relations @ Hume AI</small></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>Humans often organize large, skilled groups to undertake complex projects and then bizarrely place incompetent people in charge. Large language models (LLMs) such as OpenAI GPT-4, Anthropic Claude, and Google Gemini carry on this proud tradition with my new favorite metaphor of who has the final say in writing the text they generate—a chicken.</p>
<p><em>There is now a sequel to this article, <a href="/blog/secret-chickens-tuning">Secret LLM chickens II: Tuning the chicken</a>, if you&#x27;d like to learn how and why the &quot;chicken&quot; can be customized.</em></p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/social-card-b3347771ea10160380c43b741b943c89.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/social-card-b3347771ea10160380c43b741b943c89.jpg" alt="Cartoon chicken winking at the camera. It is in a cluttered workspace filled with multiple computer monitors, electronics, and miscellaneous items, giving the impression of a tech-savvy or hacker chicken." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>"Inside the Chicken Lab," generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Some key points I&#x27;ll address here are:</div><div class="admonitionContent_BuS1"><ul>
<li>Modern LLMs are huge and incredibly sophisticated. However, for every word they generate, they have to hand their predictions over to a simple, random function to pick the actual word.</li>
<li>This is because neural networks are deterministic, and without the inclusion of randomness, they would always produce the same output for any given prompt.</li>
<li>These random functions that choose the word are no smarter than a chicken pecking at differently-sized piles of feed to choose the word.</li>
<li>Without these &quot;stochastic chickens,&quot; large language models wouldn&#x27;t work due to problems with repetitiveness, lack of creativity, and contextual inappropriateness.</li>
<li>It&#x27;s nearly impossible to prove the originality or source of any specific piece of text generated by these models.</li>
<li>The reliance on these &quot;chickens&quot; for text generation illustrates a fundamental difference between artificial intelligence and human cognition.</li>
<li>LLMs can be viewed as either deterministic or stochastic depending on your point of view.</li>
<li>The &quot;stochastic chicken&quot; isn&#x27;t the same as the paradigm of the &quot;stochastic parrot.&quot;</li>
</ul></div></div>
<br>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>A convenient untruth</div><div class="admonitionContent_BuS1"><p>For much of this post, I use the term <em>word</em> instead of <em>token</em> to describe what an LLM predicts. Tokens can be punctuation marks, or parts of words—even capitalization can split words into multiple tokens (for example, &quot;hello&quot; is one token, but &quot;Hello&quot; might be split into two tokens of &quot;H&quot; and &quot;ello&quot;. This is a friendly simplification to avoid having to address why the model might predict half a word or a semicolon, since the underlying principles of the &quot;stochastic chicken&quot; are the same.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llms-arent-messing-around-when-they-say-theyre-large">LLMs aren&#x27;t messing around when they say they&#x27;re &quot;large.&quot;<a class="hash-link" aria-label="Direct link to LLMs aren&#x27;t messing around when they say they&#x27;re &quot;large.&quot;" title="Direct link to LLMs aren&#x27;t messing around when they say they&#x27;re &quot;large.&quot;" href="/blog/secret-chickens-llm#llms-arent-messing-around-when-they-say-theyre-large">​</a></h2>
<p>Picture a stadium full of people. Here&#x27;s <a href="https://en.wikipedia.org/wiki/Kyle_Field" target="_blank" rel="noopener noreferrer">Kyle Field</a> in College Station, Texas, with a seating capacity of 102,733. In this photo from 2015, it looks pretty full, so let&#x27;s assume there are 100,000 people there.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/kyle_field-335450615c6ec36f7b005816db27b4e4.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/kyle_field-335450615c6ec36f7b005816db27b4e4.jpg" alt="A panorama of the interior of Kyle Field (a massive basebal stadium which holds more than 100,000 people) in College Station, Texas. Taken at the Ball State game during the 2015 season." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>A panorama of the interior of Kyle Field in College Station, Texas. Taken at the Ball State game during the 2015 season. Attribution: <a href="https://en.wikipedia.org/wiki/User:Janreagan" target="_blank" rel="noopener noreferrer">Janreagan</a> at <a href="https://commons.wikimedia.org/wiki/File:Kyle_Field_Panorama.jpg" target="_blank" rel="noopener noreferrer">Wikipedia Commons</a></span></figcaption></figure>
<p>We&#x27;ve given each one of these very patient people a calculator along with the instruction that the person in the seat in front of them will give them some number, at which point they need to do a little work on their calculator and pass their new number to the person behind them. For the sake of this analogy, let&#x27;s assume that despite a considerable number of them being distracted, drunk, or children, they are all able to complete the task.</p>
<p>As the numbers travel from the front of the stadium to the back, they undergo a series of transformations. Each person&#x27;s &quot;little work&quot; on their calculator is akin to the operations performed by neurons in a layer of the neural network—applying weights (learned parameters), adding biases, and passing through activation functions. These transformations are based on the knowledge embedded in the model&#x27;s parameters, trained to recognize patterns, relationships, and the structure of language.</p>
<p>By the time the numbers reach the last person in the stadium, they have been transformed multiple times, with each step incorporating more context and adjusting the calculation based on the model&#x27;s architecture and trained parameters. This final result can be seen as the model&#x27;s output—a complex representation of the input data that encodes the probabilities of the next possible word.</p>
<p>The model&#x27;s final output isn’t a single number or word, though; it’s a list of words and probabilities, where each probability is the likelihood that that word will be the <em>next</em> word in a sentence.</p>
<p>But here&#x27;s the strange part: despite all this incredible depth of stored knowledge, we&#x27;re going to take these recommended answers provided by these thousands of people, and select the next word using a completely random process. It&#x27;s kind of like a huge pyramid where people work together to assemble a set of possible answers to a problem, then hand it at the top to a person flipping a coin.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/coinflip-bae6d6fb3437f8db985229bff06959d2.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/coinflip-bae6d6fb3437f8db985229bff06959d2.jpg" alt="Illustration of a large pyramid made of desks with people working at computers, exponentially stacked towards a single figure standing at the top under a spotlight with a moon in the background, symbolizing hierarchy or corporate structure. The man at the top is flipping a coin, illustrating the idea of a game of chance ultimately determining the output of the people." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>'Guys... I'm not sure our boss is as smart as we think he is...'</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>Actually, we can do better than this. To really illustrate the contrast between the complexity of a  model with billions of parameters getting its final answer from a dumb-as-a-rock random number generator, let&#x27;s use something truly silly.</p>
<p>Let&#x27;s use a chicken.</p>
<p>To get this chicken involved, we&#x27;re going to use words and probabilities to create piles of chicken feed, each one representing a potential next word the model might generate. The bigger the pile, the higher the probability that the associated word should be selected.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/chicken-intro-8dae2dc22acc52a6a07f4ee64c894d73.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/chicken-intro-8dae2dc22acc52a6a07f4ee64c894d73.jpg" alt="Satirical cartoon featuring a robot presenting a chicken with word options for the phrase &#x27;why did the chicken cross the ___&#x27;, with &#x27;road&#x27;, &#x27;playground&#x27;, &#x27;moon&#x27;, and &#x27;refrigerator&#x27; as choices, set against a backdrop of numerous robots working at desks." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Chickens have the amazing property of choosing their food depending on the size of the piles!</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>I am not a licensed farmer, and this is just how I assume chickens work.</p></div></div>
<p>The chicken, oblivious to the tireless efforts of the stadium&#x27;s occupants, simply wanders among the piles of chicken feed. The sizes of these piles influence its decision; larger piles are more alluring because they are simpler to spot and peck at, but the chicken also has ADHD. It&#x27;s whims or a sudden distraction might send it running to a smaller pile instead.</p>
<p>Why on earth would we do something like this? Why would we create massive, intelligent machines that ultimately rely on the random judgments of a gambling idiot?</p>
<p>The answer is that deep language models such as LLMs are built with neural networks, and neural networks are <em>deterministic.</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-deterministic-mean">What does deterministic mean?<a class="hash-link" aria-label="Direct link to What does deterministic mean?" title="Direct link to What does deterministic mean?" href="/blog/secret-chickens-llm#what-does-deterministic-mean">​</a></h3>
<p>Being deterministic means that if you do something exactly the same way every time, you&#x27;ll always get the same result. There&#x27;s no randomness or chance involved. Think of it like using a simple calculator; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 + 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span> on any basic calculator will always show &quot;4&quot;. It won&#x27;t suddenly decide to show &quot;5&quot; one day.</p>
<p>Deterministic means predictable and consistent, with no surprises based on how you started.</p>
<p>In contrast, something that&#x27;s not deterministic (we call this <em>stochastic</em>) is like rolling a die; even if you try to do it the same way each time, you can get different outcomes because chance is involved.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-are-neural-networks-deterministic">Why are neural networks deterministic?<a class="hash-link" aria-label="Direct link to Why are neural networks deterministic?" title="Direct link to Why are neural networks deterministic?" href="/blog/secret-chickens-llm#why-are-neural-networks-deterministic">​</a></h3>
<p>A neural network is a complex system inspired by the human brain, designed to recognize patterns and solve problems. It&#x27;s made up of layers of artificial neurons, which are small, simple units that perform mathematical operations. Each neuron takes in some input, applies some mathematical function to it, and then passes the result on to the next neuron in line.</p>
<p>As a huge simplification of how these models work, these neurons are organized into layers: there&#x27;s an input layer that receives the initial data (like representations of words in a sentence), one or more hidden layers that process the data further, and an output layer that provides the final decision (the probability distribution for the predicted next word in the text).</p>
<p>The diagram below might look <strong>crazy</strong> complicated, but the only thing you need to understand is that each line represents some math function performed from one circle to the next. Some numbers go in one side, and some numbers go out the other side. And the stuff that comes out on the right side will always be the same for identical stuff you put into the left side.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/nn-man-ae57cf07f43b8c1d940a682953e40d86.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/nn-man-ae57cf07f43b8c1d940a682953e40d86.jpg" alt="Artistic representation of a neural network diagram with multiple interconnected nodes in the input and hidden layers, accompanied by a perplexed man in a suit holding a coffee cup in the foreground, symbolizing human confusion on trying to understand machine learning concepts." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Maybe it looks scary, but each line connecting the dots is just a little math that never changes.</span><br><span>Generated with <a href="https://alexlenail.me/NN-SVG/" target="_blank" rel="noopener noreferrer">NN-SVG</a> and DALL-E 3</span></figcaption></figure>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>another convenient untruth</div><div class="admonitionContent_BuS1"><p>Most LLMs use a more complex neural architecture called a <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" target="_blank" rel="noopener noreferrer"><em>transformer</em></a>, but again, we&#x27;ll just simplify the idea for convenience. It makes no difference for this discussion, since transformers are also deterministic and require a chicken for generative tasks.</p></div></div>
<p>If you give the network the same input and the network has not been changed (ie., its weights, or how much it values certain pieces of input, remain the same), it will always perform the same calculations in the same order, and thus return the same output. While LLMs will have <em>billions</em> of these neurons, the basic idea is the same: for a given input, you will <strong>always</strong> get the same output. Typing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>+</mo><mn>5</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">2+5\times10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">10</span></span></span></span> into a calculator will always give you <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>52</mn></mrow><annotation encoding="application/x-tex">52</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">52</span></span></span></span>, no matter how many times you do it.</p>
<div style="font-size:1.5em;text-align:center;color:black;font-family:Roboto;margin:20px 0;padding:10px;border-left:5px solid gray"><p style="margin:0">&quot;The model&#x27;s final output isn’t a single number or word, though; it’s a list of words and probabilities, where each probability is the likelihood that that word will be the <em>next</em> word in a sentence.&quot;</p></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="so-why-do-llms-give-different-responses-each-time-to-the-same-prompt">So why do LLMs give different responses each time to the same prompt?<a class="hash-link" aria-label="Direct link to So why do LLMs give different responses each time to the same prompt?" title="Direct link to So why do LLMs give different responses each time to the same prompt?" href="/blog/secret-chickens-llm#so-why-do-llms-give-different-responses-each-time-to-the-same-prompt">​</a></h3>
<p>This seems suspicious. If you ask an LLM the same thing multiple times, it will give you different answers each time. This contradicts the claim that neural networks are deterministic, right?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="enter-the-chicken"><em>ENTER THE CHICKEN</em><a class="hash-link" aria-label="Direct link to enter-the-chicken" title="Direct link to enter-the-chicken" href="/blog/secret-chickens-llm#enter-the-chicken">​</a></h2>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/ninja-16cbfa811772cd45d7fe76886bd2940b.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/ninja-16cbfa811772cd45d7fe76886bd2940b.jpg" alt="A movie poster featuring a ninja chicken as the protagonist. This vivid depiction showcases the ninja chicken in an epic stance, wielding nunchucks, against a bustling city night. The poster&#x27;s design incorporates a mix of dark humor and action, titled &#x27;Ninja Chicken: Shadow of the Night,&#x27; with engaging taglines such as &#x27;Feathers of Fury&#x27; and &#x27;When the city sleeps, the cluck awakens.&#x27;" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>This is why we need the chicken. The chicken is stochastic, which adds randomness and unpredictability to the whole system.</p>
<p>The previously mentioned humans in the stadium, who for some reason have deified a chicken, will present the chicken with a series of words and probabilities. Technically, these are not words but <em>tokens</em>; however, for the sake of simplifying this analogy, we&#x27;ll refer to them as words. These words and probabilities can be visualized as a series of piles of chicken feed, where each word pile&#x27;s size corresponds to its probability.</p>
<p>As an example, here&#x27;s a prompt that we can give to our language model to see what next word it&#x27;s going to predict: <strong>&quot;After midnight, the cat decided to...&quot;</strong></p>
<p>If we show all the possible words that could finish this sentence, the number of food piles would match the size of the model&#x27;s vocabulary. For a model like GPT-3, this would be <strong>50,257</strong> piles of food!</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The way that GPT-3 can encapsulate all the words in 50+ languages into just 50,257 tokens is its own special magic that I&#x27;ll cover in another post. Here&#x27;s a <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener noreferrer">Wikipedia link</a> in the meantime.</p></div></div>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/chicken-50k-fca702fbe67b1291307ca13bff4495b5.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/chicken-50k-fca702fbe67b1291307ca13bff4495b5.jpg" alt="Illustration depicting a chicken looking down a long line of seed piles that extends into the horizon, giving the illusion of an infinite number of piles. The seeds are on pieces of paper with words written on them, and the size of the piles and the spacing between them decrease as they recede into the distance, enhancing the perception of infinity. The scene is elegantly composed, capturing the curiosity of the chicken as it contemplates the endless supply." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>How the cluck is an image like this going to help us understand how the chicken chooses its food?</span><br><span>Generated with OpenAI DALL-E 3</span></figcaption></figure>
<p>Clearly, trying to visualize a chicken choosing from 50,257 options isn&#x27;t very useful. Let&#x27;s instead limit our chicken&#x27;s choices to only the top 6 words that the language model chose. Here are the options for the chicken and how it might look when the chicken is presented with them:</p>
<table><thead><tr><th>Activity</th><th>blog</th><th>meditate</th><th>cook</th><th>eat</th><th>sleep</th><th>hunt</th></tr></thead><tbody><tr><td>Probability</td><td>0.03</td><td>0.05</td><td>0.1</td><td>0.15</td><td>0.2</td><td>0.25</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="after-midnight-the-cat-decided-to">&quot;After midnight, the cat decided to...&quot;<a class="hash-link" aria-label="Direct link to &quot;After midnight, the cat decided to...&quot;" title="Direct link to &quot;After midnight, the cat decided to...&quot;" href="/blog/secret-chickens-llm#after-midnight-the-cat-decided-to">​</a></h3>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/chicken-original-d7b35cc74ceb0e97b6acd0dcab8bb820.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/chicken-original-d7b35cc74ceb0e97b6acd0dcab8bb820.jpg" alt="Artistic image of a chicken examining a sequence of papers each labeled with an action (&#x27;BLOG&#x27;, &#x27;SWIM&#x27;, &#x27;COOK&#x27;, &#x27;EAT&#x27;, &#x27;SLEEP&#x27;, &#x27;HUNT&#x27;), each accompanied by a corresponding pile of seeds or grains, representing a decision-making process or prioritization based on the size of the piles." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Showing the top 6 options makes things a lot easier to understand.</span><br><span>Chicken and feed generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>Let&#x27;s suppose the chicken decides to choose the word <strong>&quot;eat,&quot;</strong> despite it only being the third most probable word since it has the third largest feed pile.</p>
<p>Unfortunately, our chicken&#x27;s work is just beginning, because it will now need to choose every subsequent word in the following way. Once again, it will be presented with a list of probable words from thousands of hardworking humans in the stadium, and once again, it will have to use its tiny chicken brain to select a word. Now our prompt to complete is <strong>&quot;After midnight, the cat decided to eat...&quot;</strong> with the following top 6 possible words and their probability distribution:</p>
<table><thead><tr><th>Food</th><th>birds</th><th>plants</th><th>grass</th><th>mice</th><th>catnip</th><th>tuna</th></tr></thead><tbody><tr><td>Probability</td><td>0.02</td><td>0.04</td><td>0.11</td><td>0.13</td><td>0.22</td><td>0.23</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="after-midnight-the-cat-decided-to-eat">&quot;After midnight, the cat decided to eat...&quot;<a class="hash-link" aria-label="Direct link to &quot;After midnight, the cat decided to eat...&quot;" title="Direct link to &quot;After midnight, the cat decided to eat...&quot;" href="/blog/secret-chickens-llm#after-midnight-the-cat-decided-to-eat">​</a></h3>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/chicken-quizzical-a711c3626bbf51c3a49852347ec4d6c5.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/chicken-quizzical-a711c3626bbf51c3a49852347ec4d6c5.jpg" alt="Cartoon chicken with a quizzical expression standing next to a line of papers each with a word (&#x27;BIRDS&#x27;, &#x27;PLANTS&#x27;, &#x27;GRASS&#x27;, &#x27;MICE&#x27;, &#x27;CATNIP&#x27;, &#x27;TUNA&#x27;) and a corresponding pile of seeds or grains, reflecting a sense of repetition or routine in making choices. The chicken is thinking &#x27;I gotta do this again?!&#x27;" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>One word down, just a few hundred more to go.</span><br><span>Chicken and feed generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>The incredible irony of all this is that some of these language models are trained using <em>trillions</em> of words—comprising the collective works of humanity—and costing <em>millions</em> of dollars in the process. We then hand these predictions over to an unintelligent gambling chicken to choose what to actually say. With each choice from the chicken, the model&#x27;s output increasingly diverges from the sequence of the most  probable words, and yet somehow, this produces more natural-sounding language.</p>
<p>It&#x27;s kind of bonkers.</p>
<p>In fact, not using the chicken—and just taking the most probable words from the LLM—generates poor language. This is called <strong>greedy decoding</strong>, and it has a whole host of problems.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-we-need-the-chicken">Why we need the chicken<a class="hash-link" aria-label="Direct link to Why we need the chicken" title="Direct link to Why we need the chicken" href="/blog/secret-chickens-llm#why-we-need-the-chicken">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="repetitiveness">Repetitiveness<a class="hash-link" aria-label="Direct link to Repetitiveness" title="Direct link to Repetitiveness" href="/blog/secret-chickens-llm#repetitiveness">​</a></h3>
<p>Not using the chicken, and just taking the most probable word (greedy decoding) tends to produce repetitive or looped text. Since it always chooses the most probable next word, it can get stuck in a pattern where the same sequence of words keeps being selected. This can happen in situations where the model finds a certain pattern of text to be highly probable and, without the chicken to encourage diversity, falls into a cycle of selecting the same sequence over and over again.</p>
<p>Here&#x27;s a short example with some creative writing:</p>
<p><strong>Prompt</strong>: &quot;Write a short story taking place in a park in the morning&quot;</p>
<p>The model begins the sentence without issue, and begins by writing &quot;The park was serene in the early morning, with&quot;, but then it runs into problems.</p>
<p><strong>Greedy Decoding Sequence</strong>:</p>
<ol>
<li>The model predicts &quot;the&quot; as the most probable next word.</li>
<li>Following &quot;the&quot;, it predicts &quot;birds&quot; as the next word.</li>
<li>After &quot;birds&quot;, it predicts &quot;were&quot; as the next word.</li>
<li>Then, &quot;singing&quot; is predicted.</li>
<li>However, after &quot;singing&quot;, the model might predict &quot;in&quot; as the next most probable word, leading back to &quot;the&quot; again, and then &quot;park&quot;, forming a loop.</li>
<li>We get this conversation:</li>
</ol>
<div style="font-family:&quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;max-width:90%;margin:auto;color:inherit"><div style="display:flex;align-items:flex-end;flex-direction:row;margin-bottom:10px"><img src="/img/userAvatar.jpeg" alt="user" style="width:40px;height:40px;border-radius:20px"><div style="background-color:#f0f0f0;color:#000;padding:10px 15px;border-radius:15px;margin:0 10px;max-width:70%"><p style="margin:0">Write a short story taking place in a park in the morning</p></div></div><div style="display:flex;align-items:flex-end;flex-direction:row-reverse;margin-bottom:10px"><img src="/img/chatbotAvatar.jpeg" alt="chatbot" style="width:40px;height:40px;border-radius:20px"><div style="background-color:#d1e7dd;color:#000;padding:10px 15px;border-radius:15px;margin:0 10px;max-width:70%"><p style="margin:0">The park was serene in the early morning, with the birds were singing in the park with the birds were singing in the park with the birds were singing in the park with the birds were singing in...</p></div></div></div>
<br>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="well-that-didnt-work">Well, that didn&#x27;t work.<a class="hash-link" aria-label="Direct link to Well, that didn&#x27;t work." title="Direct link to Well, that didn&#x27;t work." href="/blog/secret-chickens-llm#well-that-didnt-work">​</a></h4>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/looping-ab96aaa4ea83406ada5b2c1c073cc2db.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/looping-ab96aaa4ea83406ada5b2c1c073cc2db.jpg" alt="A humorous and artistic illustration featuring a comically exasperated humanoid robot at a desk. The robot, sleek and metallic, has an exaggerated expression of frustration with its circuits visibly overheating. Above the robot&#x27;s head is a large thought bubble filled with arrows spinning in endless circles, symbolizing its stuck thought loops. The scene is whimsically cluttered with papers and books under a dim desk lamp, emphasizing the robot&#x27;s humorous struggle with its thoughts." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Writing is easy is easy is easy is easy...</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lack-of-creativity-and-diversity">Lack of Creativity and Diversity<a class="hash-link" aria-label="Direct link to Lack of Creativity and Diversity" title="Direct link to Lack of Creativity and Diversity" href="/blog/secret-chickens-llm#lack-of-creativity-and-diversity">​</a></h3>
<p>Real human language is rich and varied, often taking unexpected turns, like a beautiful jellyfish riding a purple unicorn. By always choosing the most probable word, the generated text misses out on these creative and less predictable aspects of language, resulting in outputs that feel dull or formulaic.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/unicorn-015a145da2ad45b51fd116fdfd18b064.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/unicorn-015a145da2ad45b51fd116fdfd18b064.jpg" alt="An imaginative illustration, featuring a beautiful, translucent jellyfish with delicate, flowing tentacles, riding atop a majestic purple unicorn with a shimmering mane and tail. The unicorn is prancing among fluffy clouds against a sky-blue background. The unicorn is saying &#x27;Greedy decoding? Ugh...&#x27; and the jellyfish is responding &#x27;We&#x27;re not fans of it&#x27;" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>A unicorn sting operation</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="contextual-inappropriateness">Contextual Inappropriateness<a class="hash-link" aria-label="Direct link to Contextual Inappropriateness" title="Direct link to Contextual Inappropriateness" href="/blog/secret-chickens-llm#contextual-inappropriateness">​</a></h3>
<p>As weird as this sounds, the most probable next word is not always appropriate. Language is highly contextual, and the best choice in one situation might not be the most statistically common one. Greedy decoding can lead to nonsensical or awkward phrases that, while grammatically correct, do not fit well with the preceding text. Here&#x27;s another example using a job application:</p>
<p><strong>Prompt</strong>: &quot;Write me a cover letter applying for a job in marketing&quot;</p>
<p>Again, the model begins the sentence without issue, and begins by writing, &quot;Dear Ms. Smith, I am writing to express my interest ...&quot; but then again we hit a pothole.</p>
<p>Since this is a cover letter for a job application, the language model should discuss the sender&#x27;s relevant skills, experiences, or why they are a good fit for the job. However, a contextually inappropriate continuation might look like this:</p>
<div style="font-family:&quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;max-width:90%;margin:auto;color:inherit"><div style="display:flex;align-items:flex-end;flex-direction:row;margin-bottom:10px"><img src="/img/userAvatar.jpeg" alt="user" style="width:40px;height:40px;border-radius:20px"><div style="background-color:#f0f0f0;color:#000;padding:10px 15px;border-radius:15px;margin:0 10px;max-width:70%"><p style="margin:0">Write me a cover letter applying for a job in marketing</p></div></div><div style="display:flex;align-items:flex-end;flex-direction:row-reverse;margin-bottom:10px"><img src="/img/chatbotAvatar.jpeg" alt="chatbot" style="width:40px;height:40px;border-radius:20px"><div style="background-color:#d1e7dd;color:#000;padding:10px 15px;border-radius:15px;margin:0 10px;max-width:70%"><p style="margin:0">Dear Ms. Smith, I am writing to express my interest in the marketing position advertised on your website. With my extensive experience in the field, I believe I am well-qualified for the role.</p></div></div><div style="display:flex;align-items:flex-end;flex-direction:row-reverse;margin-bottom:10px"><img src="/img/chatbotAvatar.jpeg" alt="chatbot" style="width:40px;height:40px;border-radius:20px"><div style="background-color:#d1e7dd;color:#000;padding:10px 15px;border-radius:15px;margin:0 10px;max-width:70%"><p style="margin:0">In addition to my professional qualifications, I love to party on weekends and often go skydiving for fun. My favorite TV show is &#x27;The Office,&#x27; and I&#x27;m a great cook. I&#x27;m looking for an adventurous partner in crime who&#x27;s down for an impromptu trip to Vegas or a quiet evening at home watching &quot;Real Housewives&quot;.</p></div></div></div>
<br>
<p>The model, perhaps trained on more dating profiles than job applications, has continued the cover letter using the <em>most</em> probable words it predicted. While language models can recognize patterns and generate text based on statistical probabilities, they don&#x27;t understand context in the same way humans do. They might link professional qualifications to personal hobbies due to statistical correlations in the training data and not recognize a job application, which sounds a tad ridiculous.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/beach-b93c7b3d23110b51f488a823071c97b6.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/beach-b93c7b3d23110b51f488a823071c97b6.jpg" alt="A whimsical and stylish illustration depicting a humanoid robot in a business suit, walking comically with exaggerated movements along a beach. The robot is walking in the water up to its ankles, causing  splashes and waves around it. The beach scene includes soft sand and gentle waves under a serene sky. The humorous contrast of the formally attired robot deeply immersed in the water, playfully interacts with the natural seaside environment, creating a surreal and amusing visual narrative." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>I like romantic walks on the beach with someone whose love language is PowerPoint and performance bonuses.</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="inability-to-explore-multiple-paths">Inability to Explore Multiple Paths<a class="hash-link" aria-label="Direct link to Inability to Explore Multiple Paths" title="Direct link to Inability to Explore Multiple Paths" href="/blog/secret-chickens-llm#inability-to-explore-multiple-paths">​</a></h3>
<p>Greedy decoding means that an LLM will always produce the same output for any given input. Asking it to &quot;tell a story,&quot; for example, would always result in the same story, assuming it isn&#x27;t plagued with the previously mentioned problems. Language generation, especially in creative or complex tasks, often benefits from considering multiple possible continuations at each step. Greedy decoding&#x27;s linear path through the probability distribution means fewer interesting outputs that an exploratory approach could uncover.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-the-necessary-existence-of-the-chicken-imply">What does the necessary existence of the chicken imply?<a class="hash-link" aria-label="Direct link to What does the necessary existence of the chicken imply?" title="Direct link to What does the necessary existence of the chicken imply?" href="/blog/secret-chickens-llm#what-does-the-necessary-existence-of-the-chicken-imply">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="theres-probably-no-way-to-definitively-prove-that-a-given-text-was-generated">There&#x27;s probably no way to definitively prove that a given text was generated.<a class="hash-link" aria-label="Direct link to There&#x27;s probably no way to definitively prove that a given text was generated." title="Direct link to There&#x27;s probably no way to definitively prove that a given text was generated." href="/blog/secret-chickens-llm#theres-probably-no-way-to-definitively-prove-that-a-given-text-was-generated">​</a></h3>
<p>You may have heard this before since it&#x27;s been floating around the internet for a few years, but every time you shuffle a deck of playing cards, it&#x27;s almost certain that the specific order of cards has never existed before and will never exist again. In a standard deck of 52 cards, the number of possible ways to arrange the cards is 52 factorial <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>52</mn><mo stretchy="false">!</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(52!)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">52</span><span class="mclose">!)</span></span></span></span>. This number is approximately <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8.07</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>67</mn></msup></mrow><annotation encoding="application/x-tex">8.07 \times 10^{67}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">8.07</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">67</span></span></span></span></span></span></span></span></span></span></span></span>, an extraordinarily large figure.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>Because we&#x27;re now talking about the size of a model&#x27;s vocabulary, I&#x27;ll switch to using the accurate term &quot;token&quot;.</p></div></div>
<p>In the case of a language model generating a sequence of, say, 2,000 tokens, the number of possible combinations is also staggeringly high. It actually blasts right past the number of configurations of a deck of 52 playing cards!</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/cards-14db7f110be05c8c2664af1f41dc26cd.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/cards-14db7f110be05c8c2664af1f41dc26cd.jpg" alt="Dynamic image of a cascading fan of playing cards spread out in an arc with several cards floating in the air against a dark background, illustrating the seemingly infinite number of ways cards can be ordered." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>That's a lot of card combinations!</span><br><span>Generated with OpenAI DALL-E 3.</span></figcaption></figure>
<p>The logarithm (base 10) of the number of possible combinations for a sequence of 2,000 tokens, with GPT-3&#x27;s vocabulary size of 50,257 unique tokens, is approximately <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><mo separator="true">,</mo><mn>402</mn></mrow><annotation encoding="application/x-tex">9,402</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">9</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">402</span></span></span></span>. This means the total number of combinations is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>9402</mn></msup></mrow><annotation encoding="application/x-tex">10^{9402}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">9402</span></span></span></span></span></span></span></span></span></span></span></span>.</p>
<p>To put this into perspective, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>9402</mn></msup></mrow><annotation encoding="application/x-tex">10^{9402}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">9402</span></span></span></span></span></span></span></span></span></span></span></span> is an astronomically large number. It&#x27;s far beyond the total number of atoms in the observable universe (estimated to be around <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>80</mn></msup></mrow><annotation encoding="application/x-tex">10^{80}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">80</span></span></span></span></span></span></span></span></span></span></span></span>). Even considering reasonable sampling mechanisms that might drastically reduce this number, the space of possible combinations is so large that for practical purposes, it is infinite.</p>
<p>Therefore, the likelihood of generating the exact same sequence of 2,000 tokens twice is so <em>incredibly</em> small that it&#x27;s effectively zero in any practical sense. The ridiculous size of this combinatorial space basically guarantees that generated text of any length is unique.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-the-size-of-the-combinatorial-space-calculated">How is the size of the combinatorial space calculated?<a class="hash-link" aria-label="Direct link to How is the size of the combinatorial space calculated?" title="Direct link to How is the size of the combinatorial space calculated?" href="/blog/secret-chickens-llm#how-is-the-size-of-the-combinatorial-space-calculated">​</a></h4>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Math &amp; Python code (optional technical content)</summary><div><div class="collapsibleContent_i85q"><div><p>Given a vocabulary size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span> and a sequence length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span>, the total number of possible combinations can be calculated as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">V^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span></span></span></span></span></span></span>. For a vocabulary size of 50,257 and a sequence length of 2,000, the calculation is as follows:</p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Total Combinations</mtext><mo>=</mo><msup><mi>V</mi><mi>N</mi></msup><mo>=</mo><mn>50</mn><mo separator="true">,</mo><mn>25</mn><msup><mn>7</mn><mn>2000</mn></msup></mrow><annotation encoding="application/x-tex">\text{Total Combinations} = V^N = 50,257^{2000}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord text"><span class="mord">Total Combinations</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8913em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0585em;vertical-align:-0.1944em"></span><span class="mord">50</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">25</span><span class="mord"><span class="mord">7</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2000</span></span></span></span></span></span></span></span></span></span></span></span></span><p>However, this number is astronomically large and beyond direct calculation. Instead, we use logarithms to estimate this number. The logarithm (base 10) of the total number of combinations is calculated as:</p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>10</mn></msub><mo stretchy="false">(</mo><mtext>Total Combinations</mtext><mo stretchy="false">)</mo><mo>=</mo><mi>N</mi><mo>⋅</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>10</mn></msub><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2000</mn><mo>⋅</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>10</mn></msub><mo stretchy="false">(</mo><mn>50</mn><mo separator="true">,</mo><mn>257</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_{10}(\text{Total Combinations}) = N \cdot \log_{10}(V) = 2000 \cdot \log_{10}(50,257)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord text"><span class="mord">Total Combinations</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2000</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">50</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">257</span><span class="mclose">)</span></span></span></span></span><p>Here&#x27;s how we can calculate this with Python:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">Python code for calculating the combinatorial space of a 2,000 token sequence with a 50,257 token vocabulary</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> math</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Assuming a reasonable vocabulary size for a language model</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># For simplicity, let&#x27;s take GPT-3&#x27;s vocabulary size of 50,257 unique tokens</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vocabulary_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">50257</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Number of tokens in the sequence</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sequence_length </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Calculating the number of possible combinations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Since each token can be any one of the 50,257, for a sequence of 2,000 tokens,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># the total number of  combinations would be vocabulary_size ** sequence_length</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># However, this number is astronomically large and beyond what can be reasonably calculated directly.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Instead, we will use logarithms to estimate this.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Calculate the logarithm (base 10) of the number of combinations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">log_combinations </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> sequence_length </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> math</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">log10</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">vocabulary_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">log_combinations</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Output</strong></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">9402.393121225747</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></div></details>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="despite-llms-appearing-to-think-like-people-the-need-for-a-stochastic-chicken-is-an-argument-for-why-they-dont">Despite LLMs appearing to think like people, the need for a stochastic chicken is an argument for why they don&#x27;t.<a class="hash-link" aria-label="Direct link to Despite LLMs appearing to think like people, the need for a stochastic chicken is an argument for why they don&#x27;t." title="Direct link to Despite LLMs appearing to think like people, the need for a stochastic chicken is an argument for why they don&#x27;t." href="/blog/secret-chickens-llm#despite-llms-appearing-to-think-like-people-the-need-for-a-stochastic-chicken-is-an-argument-for-why-they-dont">​</a></h3>
<p>The use of the chicken in LLMs indeed highlights a fundamental difference between how these models generate text, and how humans think and produce language.</p>
<p>LLMs generate text based on statistical patterns learned from vast amounts of data, and the chicken introduces randomness as a mechanism to produce diverse and contextually appropriate responses. In contrast, cognitive processes like memory, reasoning, and emotional context, which do not rely on statistical sampling in the same way, are what drive human thought and language production.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/hidden-chicken-cb6f7171443831ef9cd68613fdfdffbc.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/hidden-chicken-cb6f7171443831ef9cd68613fdfdffbc.jpg" alt="An illustration depicting a humorous scene where a chicken, cleverly disguised or attempting to blend in, is hiding amongst a crowd of business people wearing trench coats. The setting is an urban street scene, perhaps near a bustling business district or a subway entrance, during a busy morning or evening commute. The business people are depicted in a range of poses typical of busy city life, such as reading newspapers, checking smartphones, and holding cups of coffee, all unaware of the chicken among them." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Nothing to see here.</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>In my opinion, the chicken is a stopgap, a temporary bandaid to solve a problem, and it does not imbue the model with understanding or cognition. This distinction is central to ongoing discussions in AI about the nature of intelligence, consciousness, and the difference between simulating aspects of human thought and actually replicating the underlying cognitive processes.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="whether-llms-are-deterministic-or-stochastic-depends-on-your-point-of-view">Whether LLMs are deterministic or stochastic depends on your point of view.<a class="hash-link" aria-label="Direct link to Whether LLMs are deterministic or stochastic depends on your point of view." title="Direct link to Whether LLMs are deterministic or stochastic depends on your point of view." href="/blog/secret-chickens-llm#whether-llms-are-deterministic-or-stochastic-depends-on-your-point-of-view">​</a></h3>
<p>Your perspective on this will change depending on whether you focus on the neural network architecture itself or instead consider the complete text generation process, which includes our beloved chicken.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llms-as-deterministic-systems">LLMs as deterministic systems<a class="hash-link" aria-label="Direct link to LLMs as deterministic systems" title="Direct link to LLMs as deterministic systems" href="/blog/secret-chickens-llm#llms-as-deterministic-systems">​</a></h4>
<p>The transformer architecture and the learned parameters of the model are fixed once the model is trained. Given the same input sequence and model parameters, the network will always produce the same output, which, in the case of LLMs, is a distribution over the next possible tokens. The computation through the network&#x27;s layers and the resulting probabilities for the next token are entirely predictable and repeatable. In this view, the distribution of tokens produced by the model represents the entire system, and the chicken is a part of some other system.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llms-as-stochastic-systems">LLMs as stochastic systems<a class="hash-link" aria-label="Direct link to LLMs as stochastic systems" title="Direct link to LLMs as stochastic systems" href="/blog/secret-chickens-llm#llms-as-stochastic-systems">​</a></h4>
<p>When considering the model as a complete system for generating text, the inclusion of the chicken  as an integral part of its operation means that the overall system behaves in a stochastic manner. The stochastic chicken introduces randomness, which results in different text sequences in different runs with the same input prompt. With this view, since text cannot be generated without the chicken, the chicken is an integral part of the system.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="combined-perspective-deterministic-core-of-a-stochastic-system">Combined perspective: deterministic core of a stochastic system<a class="hash-link" aria-label="Direct link to Combined perspective: deterministic core of a stochastic system" title="Direct link to Combined perspective: deterministic core of a stochastic system" href="/blog/secret-chickens-llm#combined-perspective-deterministic-core-of-a-stochastic-system">​</a></h4>
<p>It&#x27;s probably easiest to view LLMs as deterministic systems with respect to their neural network computation, producing a predictable set of output probabilities for the next token when given an input. However, when considering the complete text generation process, which includes the decision-making by a silly bird of some kind, the system behaves stochastically.</p>
<p>It&#x27;s still mind-bending that these models that have fundamentally changed the world rely on pure chance for their final answers, and so far, this is the best method we have.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/dice-55a2fa83ffcc9173d8a97cbedd99835b.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/dice-55a2fa83ffcc9173d8a97cbedd99835b.jpg" alt="Monochrome illustration depicting rows of people seated at desks in a vast grid, working very hard to deliver information to the head table. At the head table, people are playing a dice game." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>God may not play dice with the universe, but language models sure do.</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="is-a-stochastic-chicken-the-same-as-a-stochastic-parrot">Is a &quot;stochastic chicken&quot; the same as a &quot;stochastic parrot&quot;?<a class="hash-link" aria-label="Direct link to Is a &quot;stochastic chicken&quot; the same as a &quot;stochastic parrot&quot;?" title="Direct link to Is a &quot;stochastic chicken&quot; the same as a &quot;stochastic parrot&quot;?" href="/blog/secret-chickens-llm#is-a-stochastic-chicken-the-same-as-a-stochastic-parrot">​</a></h2>
<p>You may have heard the term &quot;stochastic parrot&quot; as a way of saying that LLMs don&#x27;t <em>understand</em> what they&#x27;re actually saying.</p>
<p>The term was coined in the paper <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922" target="_blank" rel="noopener noreferrer"><em>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</em></a> by Bender et al, in which they argue that LLMs probabilistically link words and sentences together without considering meaning.</p>
<p>However, despite the fact that both chickens and parrots are birds, they don&#x27;t refer to the same thing.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/parrot-vs-chicken-1fdb0148bb218fbf3ccb25f5f09f5965.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/parrot-vs-chicken-1fdb0148bb218fbf3ccb25f5f09f5965.jpg" alt="A cartoon illustration depicting a parrot and a chicken facing off with aggressive and exaggerated expressions. The parrot, vibrant with multicolored feathers, and the chicken, robust and fluffy, are  close together, their beaks almost touching, highlighting the intensity of their confrontation." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>A stochastic parrot is no stochastic chicken.</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p>The &quot;stochastic parrot&quot; refers to the neural network part of the LLM rather than the sampling process. That is, it refers to the <strong>stadium full of people working together,</strong> rather than the chicken. The idea of the stochastic parrot critiques a language model&#x27;s operation in terms of how it processes and generates language, based purely on the statistical patterns observed in the training data.</p>
<p>It claims that:</p>
<ol>
<li>
<p>LLMs, like parrots, mimic words and phrases without understanding their meanings. It posits that <em>neural networks</em> regurgitate large chunks of learned text based on probabilities derived from their training data rather than on semantic understanding or reasoning.</p>
</li>
<li>
<p>Biases found in the training data are perpetuated on an enormous scale, and the cost of training such large models damages the environment.</p>
</li>
</ol>
<p>A lot of this debate hinges on philosophical ideas of what &quot;understanding&quot; and &quot;reasoning&quot; even mean, and there&#x27;s a comprehensive <a href="https://en.wikipedia.org/wiki/Stochastic_parrot" target="_blank" rel="noopener noreferrer">Wikipedia article</a> on it if you&#x27;re interested in reading more. It is indeed a bit ironic that the authors of the &quot;stochastic parrot&quot; paper named it after a &quot;random&quot; parrot, when their primary criticism of the models deals with the deterministic neural network component.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-parrot-chicken-partnership">The parrot-chicken partnership<a class="hash-link" aria-label="Direct link to The parrot-chicken partnership" title="Direct link to The parrot-chicken partnership" href="/blog/secret-chickens-llm#the-parrot-chicken-partnership">​</a></h3>
<p>I&#x27;ve used LLMs daily now for a year and a half, and in my opinion, the idea that they&#x27;re merely mimicking answers is overly simplistic. While the concerns of bias and environmental damage are valid, I&#x27;ve seen GPT-4, Claude 3 Opus, and Google Gemini perform sophisticated forms of reasoning, and it&#x27;s dismissive and naive to call these models mere parrots. That&#x27;s just my opinion, but the whole &quot;stochastic parrot&quot; thing is also a matter of opinion and a topic of ongoing research.</p>
<p>In contrast, the sampling strategies of the stochastic chicken very much exist and are a known part of how LLMs generate text, independent of how the neural network model provides the distribution of words to choose from.</p>
<p>So LLMs might be stochastic parrots—or they might not—but either way, a chicken is ultimately in charge.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/partners-1d0b9c5614f9b60f92fa25b59c5c59a8.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/partners-1d0b9c5614f9b60f92fa25b59c5c59a8.jpg" alt="A cartoon illustration featuring a parrot and a chicken as comical business partners, shaking hands in a show of cooperation. The parrot, vibrant with multicolored feathers, and the chicken, fluffy and dignified, are standing upright, each extending a wing as if they were human hands, meeting in a firm handshake. They wear small, humorous expressions of smug satisfaction, portraying a successful partnership." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Every stochastic parrot needs its stochastic chicken.</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<p><em>This topic is continued in my next article, <a href="/blog/secret-chickens-tuning">Secret chickens II: Tuning the chicken,
</a> which discusses techniques that affect the behaviour of the chicken.</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-takeaways">Key Takeaways<a class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" href="/blog/secret-chickens-llm#key-takeaways">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><ul>
<li>
<p><strong>The Stochastic Chicken</strong>: The &quot;stochastic chicken&quot; metaphor effectively illustrates how randomness is essential in text generation by LLMs, contrasting the sophistication of neural computations with the simplicity of random choice mechanisms.</p>
</li>
<li>
<p><strong>Neural Determinism and Stochastic Outputs</strong>: The neural network parts of LLMs are deterministic, meaning they always produce the same outputs for given inputs. However, the chicken adds randomness to the process of choosing the final output, which makes the whole system stochastic.</p>
</li>
<li>
<p><strong>Purpose of Randomness</strong>: The randomness introduced by the stochastic chicken is needed to prevent issues like repetitiveness and lack of creativity in generated text. It ensures that LLM outputs are diverse and not just the most statistically likely continuations.</p>
</li>
<li>
<p><strong>Human vs. AI Cognition</strong>: The reliance on stochastic processes (like the chicken) to finalize outputs highlights fundamental differences between artificial intelligence and human cognitive processes, emphasizing that AI may not &quot;think&quot; or &quot;understand&quot; in human-like ways despite producing human-like text.</p>
</li>
<li>
<p><strong>Deterministic vs. Stochastic Views</strong>: Depending on the focus—on the neural network alone or on the complete text generation process, including randomness—LLMs can be viewed as either deterministic or stochastic systems.</p>
</li>
<li>
<p><strong>Uniqueness of Generated Text</strong>: Given the vast combinatorial possibilities of token sequences in LLMs, any substantial text generated is virtually guaranteed to be unique, underscoring the impact of the chicken and making it nearly impossible to <em>prove</em> plagiarism.</p>
</li>
<li>
<p><strong>Stochastic Chicken vs. Stochastic Parrot</strong>: The &quot;stochastic chicken&quot; (the randomness in selecting outputs) and the &quot;stochastic parrot&quot; (the critique of LLMs as merely mimicking patterns from data without understanding) are not the same thing. Whether LLMs are parrots is somewhat of a conjecture, but LLMs absolutely do contain a metaphoric chicken pecking away.</p>
</li>
</ul></div></div></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/distribution">distribution</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/sampling">sampling</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/agi">AGI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/asi">ASI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatgpt">chatgpt</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chat">chat</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ml">ML</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbot">chatbot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chatbots">chatbots</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stochastic-chicken">stochastic chicken</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/stochastic-parrot">stochastic parrot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai-explained">AIExplained</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/secret-chickens-tuning"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Secret LLM chickens II: Tuning the chicken</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/llm-forward-thinking"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">LLMs are forward thinkers, and that&#x27;s a bit of a problem</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#llms-arent-messing-around-when-they-say-theyre-large">LLMs aren&#39;t messing around when they say they&#39;re &quot;large.&quot;</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#what-does-deterministic-mean">What does deterministic mean?</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#why-are-neural-networks-deterministic">Why are neural networks deterministic?</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#so-why-do-llms-give-different-responses-each-time-to-the-same-prompt">So why do LLMs give different responses each time to the same prompt?</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#enter-the-chicken"><em>ENTER THE CHICKEN</em></a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#after-midnight-the-cat-decided-to">&quot;After midnight, the cat decided to...&quot;</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#after-midnight-the-cat-decided-to-eat">&quot;After midnight, the cat decided to eat...&quot;</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#why-we-need-the-chicken">Why we need the chicken</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#repetitiveness">Repetitiveness</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#lack-of-creativity-and-diversity">Lack of Creativity and Diversity</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#contextual-inappropriateness">Contextual Inappropriateness</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#inability-to-explore-multiple-paths">Inability to Explore Multiple Paths</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#what-does-the-necessary-existence-of-the-chicken-imply">What does the necessary existence of the chicken imply?</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#theres-probably-no-way-to-definitively-prove-that-a-given-text-was-generated">There&#39;s probably no way to definitively prove that a given text was generated.</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#despite-llms-appearing-to-think-like-people-the-need-for-a-stochastic-chicken-is-an-argument-for-why-they-dont">Despite LLMs appearing to think like people, the need for a stochastic chicken is an argument for why they don&#39;t.</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#whether-llms-are-deterministic-or-stochastic-depends-on-your-point-of-view">Whether LLMs are deterministic or stochastic depends on your point of view.</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#is-a-stochastic-chicken-the-same-as-a-stochastic-parrot">Is a &quot;stochastic chicken&quot; the same as a &quot;stochastic parrot&quot;?</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#the-parrot-chicken-partnership">The parrot-chicken partnership</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/secret-chickens-llm#key-takeaways">Key Takeaways</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/kelkulus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/iankelk" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Blog Feeds</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://kelk.ai/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">RSS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://kelk.ai/blog/atom.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">Atom<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://kelk.ai/blog/feed.json" target="_blank" rel="noopener noreferrer" class="footer__link-item">JSON<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Short Attention Blog</div></div></div></footer></div>
</body>
</html>