<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">Open Source Inference at Full Throttle: Exploring TGI and vLLM | Short Attention</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kelk.ai/blog/inference-engines"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Open Source Inference at Full Throttle: Exploring TGI and vLLM | Short Attention"><meta data-rh="true" name="description" content="Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI’s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn’t be changed. While this was necessary due to the model&#x27;s enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required."><meta data-rh="true" property="og:description" content="Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI’s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn’t be changed. While this was necessary due to the model&#x27;s enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required."><meta data-rh="true" property="og:image" content="https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true"><meta data-rh="true" name="twitter:image" content="https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-09-28T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://kelk.ai"><meta data-rh="true" property="article:tag" content="LLM,ChatGPT,AI,vLLM,PagedAttention,LocalAI,OpenWeightsModels,LlamaModels,InferenceEngine,KVCache"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://kelk.ai/blog/inference-engines"><link data-rh="true" rel="alternate" href="https://kelk.ai/blog/inference-engines" hreflang="en"><link data-rh="true" rel="alternate" href="https://kelk.ai/blog/inference-engines" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://kelk.ai/blog/inference-engines","mainEntityOfPage":"https://kelk.ai/blog/inference-engines","url":"https://kelk.ai/blog/inference-engines","headline":"Open Source Inference at Full Throttle: Exploring TGI and vLLM","name":"Open Source Inference at Full Throttle: Exploring TGI and vLLM","description":"Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI’s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn’t be changed. While this was necessary due to the model's enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required.","datePublished":"2024-09-28T00:00:00.000Z","author":{"@type":"Person","name":"Ian Kelk","description":"Developer Relations","url":"https://kelk.ai","image":"https://github.com/iankelk.png"},"image":{"@type":"ImageObject","@id":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true","url":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true","contentUrl":"https://github.com/iankelk/iankelk.github.io/blob/main/blog/2024-09-28-inference-engines/social-card.jpg?raw=true","caption":"title image for the blog post: Open Source Inference at Full Throttle: Exploring TGI and vLLM"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://kelk.ai/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Short Attention RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Short Attention Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="Short Attention JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PNHB98RZ0D"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PNHB98RZ0D",{anonymize_ip:!0})</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.049cba7a.css">
<script src="/assets/js/runtime~main.756d5d5f.js" defer="defer"></script>
<script src="/assets/js/main.d6cf883c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Short Attention Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Short Attention Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Projects Page</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Short Attention Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/iankelk" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/inference-engines">Open Source Inference at Full Throttle: Exploring TGI and vLLM</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/secret-chickens-tuning">Secret LLM chickens II: Tuning the chicken</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/secret-chickens-llm">The secret chickens that run LLMs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/llm-forward-thinking">LLMs are forward thinkers, and that&#x27;s a bit of a problem</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/how-chatgpt-fools-us">How ChatGPT fools us into thinking we&#x27;re having a conversation</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">Open Source Inference at Full Throttle: Exploring TGI and vLLM</h1><div class="container_mt6G margin-vert--md"><time datetime="2024-09-28T00:00:00.000Z">September 28, 2024</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://kelk.ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/iankelk.png" alt="Ian Kelk"></a><div class="avatar__intro"><div class="avatar__name"><a href="https://kelk.ai" target="_blank" rel="noopener noreferrer"><span>Ian Kelk</span></a></div><small class="avatar__subtitle">Developer Relations</small></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>Large language models (LLMs) have received a huge amount of attention ever since ChatGPT first appeared at the end of 2022. ChatGPT represented a notable breakthrough in AI language models, surprising everyone with its ability to generate human-like text. However, it came with a notable limitation: the model could only be accessed via OpenAI’s servers. Users could interact with ChatGPT through a web interface, but they lacked access to the underlying architecture and model weights. Although a few months later OpenAI added access to the underlying GPT-3.5 model to its API, the models still resided on remote servers, and the underlying weights of the models couldn’t be changed. While this was necessary due to the model&#x27;s enormous computational requirements, it naturally raised questions about privacy and access since all data could be read by OpenAI and an external internet connection was required.</p>
<p>Two years later and the situation has dramatically changed. Thanks to the rise of open-weights alternatives like Meta’s Llama models, we now have multiple options for running LLMs locally on our own hardware. Access is no longer tethered to cloud-based infrastructures, but instead users can directly manipulate, explore, and deploy models themselves.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/social-card-ac5de118d9a61de15beb773dc746b98b.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/social-card-ac5de118d9a61de15beb773dc746b98b.jpg" alt="Two old-timey F1 race cars labeled TGI and vLLM, capturing that vintage racing vibe with a touch of futuristic flair. This design emphasizes the competitive spirit between the two inference engines, set in a nostalgic, dynamic scene." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Some key points I&#x27;ll address here are:</div><div class="admonitionContent_BuS1"><ul>
<li>The transition from server-based LLMs like ChatGPT to locally runnable models, enabling customization and offline usage.</li>
<li>The role of inference engines in executing neural networks using learned parameters for local model inference.</li>
<li>Introduction to <strong>PagedAttention</strong> in vLLM, improving memory efficiency through better key-value cache management.</li>
<li>A comparison of TGI and vLLM, highlighting shared features such as tensor parallelism and batching, and distinct features like speculative decoding and structured output guidance.</li>
<li>Explanation of <strong>latency</strong> and <strong>throughput</strong>, including how these performance metrics influence LLM deployments.</li>
<li>Advice on selecting between TGI and vLLM based on specific enterprise needs, focusing on use case experimentation and benchmarking.</li>
<li>An overview of licensing differences, discussing TGI&#x27;s shift back to <strong>Apache 2.0</strong>, aligning with vLLM’s license.</li>
<li>Practical code examples showing how to deploy models using both TGI and vLLM for real-world applications.</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-llm-inference-engine">What is an LLM inference engine?<a class="hash-link" aria-label="Direct link to What is an LLM inference engine?" title="Direct link to What is an LLM inference engine?" href="/blog/inference-engines#what-is-an-llm-inference-engine">​</a></h2>
<p>At its most basic level, running a local model like Meta’s Llama 3.1 70B could involve two core files: the parameters file and a piece of code to execute those parameters. The parameters, also called the model’s “weights,” are the values that the model has learned during training. For instance, the Llama 3.1 70B model has 70 billion parameters, and each of these parameters is stored as a 2-byte value. This results in a parameter file that’s about 140 GB in size. This file is essentially a list of 70 billion values, and it is not executable on its own.</p>
<p>To actually run the model, you need a small piece of code—often written in C or Python (or a mix of both)—that defines how to interpret and use those weights to generate text. This code file, the inference engine, holds the neural network architecture and algorithm that process inputs and generate outputs based on the parameters.</p>
<p>Using just these two files—the parameters and the code—you can run the model directly on your local machine. You can compile the code, point it at the parameters file, and begin interacting with the model. For example, you could prompt it to generate a poem or an essay, just as you would with cloud-based models like ChatGPT.</p>
<p>Unlike proprietary models locked behind web interfaces, open-weights models allow for customization, experimentation, and offline usage, making it possible to run LLMs in entirely self-contained and private environments.</p>
<p>While the code to run these models can be extremely short, as in Andrej Karpathy&#x27;s video, where he points to one that&#x27;s a mere 500 lines, running a model as large as Llama 3.1 70B locally requires significant computational power.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/andrej-aa31302b343b5abbbf647be2931fea67.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/andrej-aa31302b343b5abbbf647be2931fea67.jpg" alt="Two old-timey F1 race cars labeled TGI and vLLM, capturing that vintage racing vibe with a touch of futuristic flair. This design emphasizes the competitive spirit between the two inference engines, set in a nostalgic, dynamic scene." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>A still from Andrej Karpathy's video <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g" target="_blank" rel="noopener noreferrer">The busy person's intro to LLMs</a> showing the parameters of Meta's Llama 2 70B model, run.c file, an air-gapped MacBook, and a brief sample of generated text.</span></figcaption></figure>
<p>A model of that size in its raw format would need 140 GB of GPU memory to run at a realistically usable speed, and that could require two 80 GB GPUs costing tens of thousands of dollars. That’s not all they require: they need even more memory for activations, attention mechanisms, and intermediate computations.
These requirements are only for a single user! For enterprise use, the model would need to serve many users concurrently, requiring a ton of GPUs for production-scale services. This is where robust LLM inference engines, such as <a href="https://docs.vllm.ai/en/latest/" target="_blank" rel="noopener noreferrer">vLLM</a> and <a href="https://huggingface.co/docs/text-generation-inference/en/index" target="_blank" rel="noopener noreferrer">Hugging Face Text Generation Inference (TGI)</a>, become necessary.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vllm-and-pagedattention">vLLM and PagedAttention<a class="hash-link" aria-label="Direct link to vLLM and PagedAttention" title="Direct link to vLLM and PagedAttention" href="/blog/inference-engines#vllm-and-pagedattention">​</a></h2>
<p>A natural spot to start the discussion is with vLLM and the invention of <a href="https://blog.vllm.ai/2023/06/20/vllm.html" target="_blank" rel="noopener noreferrer">PagedAttention</a>, originally published in the paper <a href="https://arxiv.org/abs/2309.06180" target="_blank" rel="noopener noreferrer">“Efficient Memory Management for Large Language Model Serving with PagedAttention.”</a></p>
<p>PagedAttention solves the problem of inefficient memory management in LLM inference engines. LLMs are autoregressive, meaning the model predicts the next token in a sequence based on all the tokens that have come before it.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/chatgpt-photosynthesis-2-f3fb095ef8519a0ef8b7681e7719670c.jpeg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/chatgpt-photosynthesis-2-f3fb095ef8519a0ef8b7681e7719670c.jpeg" alt="A black and white comic strip showing a dialogue between a woman and a person wearing a &#x27;CHATGPT&#x27; shirt. The woman is labeled &#x27;USER&#x27; and asks, &#x27;What is photosynthesis?&#x27; The person labeled &#x27;CHATGPT&#x27; answers, &#x27;Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.&#x27; The woman, termed &#x27;USER&#x27; again, follows with, &#x27;Can humans do it?&#x27; and receives the reply, &#x27;No, humans cannot perform photosynthesis.&#x27; Both characters are drawn as cartoons." style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>The actual content of a converstaion with ChatGPT simulating that it remembers what was previously said.</span><br><span>People images by OpenAI DALL-E 3. Text and comic bubbles by author.</span></figcaption></figure>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>What it means to be autoregressive</div><div class="admonitionContent_BuS1"><p>Notice that when the woman asks her second question, she has to reiterate the entire previous conversation, complete with tags on who said what. LLMs require using their own outputs, plus the prompts that generated these outputs, to be prepended to the start of every new prompt from the user.</p></div></div>
<p>To do this efficiently, we need to cache certain values used to calculate self-attention scores so that the model isn’t recalculating the same things over and over. The KV cache (Key-Value cache) is a memory structure used in transformer models during the process of autoregressive text generation. It stores the intermediate states (keys and values) from previous tokens, which the model references when generating new tokens.</p>
<p>Traditional systems pre-allocate large, contiguous blocks of memory for the KV cache based on a maximum possible sequence length, even though many requests use much less memory. PagedAttention works by borrowing ideas from how operating systems manage virtual memory through paging. Instead of allocating a large contiguous block of memory for each request, it breaks the KV cache into smaller, fixed-sized blocks (or “pages”). These blocks are stored in non-contiguous memory locations.</p>
<p>To understand how PagedAttention can solve this problem, imagine you’re managing a hotel with rooms, and each guest who checks in represents a token in a sequence that needs memory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="contiguous-memory-traditional-memory-allocation">Contiguous Memory (Traditional Memory Allocation)<a class="hash-link" aria-label="Direct link to Contiguous Memory (Traditional Memory Allocation)" title="Direct link to Contiguous Memory (Traditional Memory Allocation)" href="/blog/inference-engines#contiguous-memory-traditional-memory-allocation">​</a></h3>
<p>In this system, each guest has made a reservation for a certain number of hotel rooms for themselves and their friends, and the hotel management insists that every guest’s rooms must be placed next to each other in a straight line.</p>
<p>When the guest arrives, the hotel reserves one long row of adjacent rooms. This is acceptable if the guest knows exactly how many rooms they need, but they often don&#x27;t—they have friends who either don&#x27;t show or friends who bring other friends, so they end up needing fewer or more rooms than expected.</p>
<ul>
<li>
<p><strong>Problem 1:</strong> If you reserve 6 rooms but the guest only needs 4, the extra 2 rooms go unused (this is called <em>internal fragmentation</em>).</p>
</li>
<li>
<p><strong>Problem 2:</strong> If another unexpected guest arrives but the remaining free rooms are scattered across different floors, you can’t check them in because the rooms aren’t in one long line. They’ll have to wait, even though you have enough rooms, but not in a contiguous block (this is called <em>external fragmentation</em>).</p>
</li>
</ul>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/sequential-df783d324f8d6a36f42b1acffd4c76de.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/sequential-df783d324f8d6a36f42b1acffd4c76de.jpg" alt="Guest 1 reserves 6 sequential rooms, uses 4. Guest 2 reserves 6 sequential rooms, uses 3. Rooms 5, 6, 10, 11, and 12 are wasted. Guest 3 is unable to check in" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Rooms 1 to 12 in a hotel with sequential contiguous memory allocation accomodating 2 guests with 5 wasted rooms and unable to fit a third guest.</span></figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pagedattention-paged-memory-allocation">PagedAttention (Paged Memory Allocation)<a class="hash-link" aria-label="Direct link to PagedAttention (Paged Memory Allocation)" title="Direct link to PagedAttention (Paged Memory Allocation)" href="/blog/inference-engines#pagedattention-paged-memory-allocation">​</a></h3>
<p>Now you run the hotel like an apartment complex, where the rooms don’t need to be next to each other. Instead, guests can be accommodated in separate rooms throughout the building.  Each guest is given a page in the hotel ledger that tracks where their rooms are located throughout the hotel. A guest needing 5 rooms can be allocated 2 rooms on one floor and 3 rooms on another. The hotel’s management keeps track of which rooms belong to which guest using the page.</p>
<p>This system avoids wasting rooms and allows more guests to check in.</p>
<ul>
<li>
<p><strong>Avoids Problem 1:</strong> No internal fragmentation. If a guest needs five rooms, they get exactly five, with no extra.</p>
</li>
<li>
<p><strong>Avoids Problem 2:</strong> No external fragmentation: Guests don’t need to wait for a block of adjacent rooms to become available—they can take rooms anywhere. Without trying to fit all the rooms into one long line, you can fill in the gaps and maximize space.</p>
</li>
</ul>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/paged-b531baf91d73108133090f7951f8a25d.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/paged-b531baf91d73108133090f7951f8a25d.jpg" alt="Guest 1 is using rooms 1, 3, 4, and 8. Guest 2 is using rooms 2, 9, and 11. Guest 3 is using rooms 5, 6, 7, and 10. Room 12 is available" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Rooms 1 to 12 in a hotel with paged memory allocation accomodating 3 guests with no wasted memory.</span></figcaption></figure>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Other benefits</div><div class="admonitionContent_BuS1"><p>There are also additional benefits to PagedAttention. Memory can be shared more efficiently between different processes or tasks, similar to how multiple guests could share a communal space instead of requiring separate ones.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-vllm-compare-to-tgi">How does vLLM compare to TGI?<a class="hash-link" aria-label="Direct link to How does vLLM compare to TGI?" title="Direct link to How does vLLM compare to TGI?" href="/blog/inference-engines#how-does-vllm-compare-to-tgi">​</a></h2>
<p>When vLLM was first released, it outperformed TGI by a solid margin due to PagedAttention, performing with up to 2.2x to 3.5x higher throughput.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/comparison-1f235c6948edbb826bea190711cfcbe1.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/comparison-1f235c6948edbb826bea190711cfcbe1.jpg" alt="Comparison with HuggingFace and TGI (06/2023. Up to 24x higher throughput than HuggingFace (HF). Up to 3.5x higher throughput than Text Generation Inference" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>Still from video <a href="https://www.youtube.com/watch?v=5ZlavKF_98U" target="_blank" rel="noopener noreferrer">Fast LLM Serving with vLLM and PagedAttention</a></span></figcaption></figure>
<p>However, TGI soon incorporated PagedAttention, mentioning it in their GitHub repository in <a href="https://github.com/huggingface/text-generation-inference/commit/e74bd41e0f279ab569cf6a65ac3e2cea50e80d39" target="_blank" rel="noopener noreferrer">June 2023</a>.</p>
<p>Currently, it’s quite tricky to make definitive statements about which inference engine is superior. LLM inference optimization is a rapidly evolving and heavily researched field; the best inference backend available today might quickly be surpassed by another as new optimizations become available.</p>
<p>Even more so, the web is becoming inundated—rather ironically—with LLM-generated garbage articles that frequently tout information that was already out-of-date when they were written. Even the product pages for vLLM and TGI themselves are not complete; when I was initially putting together the below table, I used <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener noreferrer">TGI&#x27;s</a> and <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM&#x27;s</a> GitHub readme files; however neither of them list all their supported quantizations. I had to check their respective documentation sites.</p>
<p>Nevertheless, you can clearly see that for many of these optimizations, the two engines are very similar.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="commonalities">Commonalities<a class="hash-link" aria-label="Direct link to Commonalities" title="Direct link to Commonalities" href="/blog/inference-engines#commonalities">​</a></h3>
<table><thead><tr><th><strong>Feature</strong></th><th><strong>TGI</strong></th><th><strong>vLLM</strong></th></tr></thead><tbody><tr><td><strong>Model Serving</strong></td><td>Simple launcher to serve most popular LLMs</td><td>Seamless integration with Hugging Face models</td></tr><tr><td><strong>Parallelism</strong></td><td>Tensor parallelism for faster inference on multiple GPUs</td><td>Tensor and pipeline parallelism for distributed inference</td></tr><tr><td><strong>Batching</strong></td><td>Continuous batching of incoming requests for high throughput</td><td>Continuous batching of incoming requests for high throughput</td></tr><tr><td><strong>Attention Optimization</strong></td><td>Optimized transformers code with FlashAttention and Paged Attention</td><td>Efficient memory management with PagedAttention and FlashAttention</td></tr><tr><td><strong>Quantization</strong></td><td>GPTQ, AWQ, bitsandbytes, EETQ, Marlin, EXL2, and FP8 quantization</td><td>GPTQ, AWQ, bitsandbytes, Marlin, AQLM, DeepSpeedFP, GGUF, INT4, INT8, FP8 quantization</td></tr><tr><td><strong>Streaming Outputs</strong></td><td>Token streaming using Server-Sent Events (SSE)</td><td>Streaming outputs</td></tr><tr><td><strong>Hardware Support</strong></td><td>Supports <strong>NVIDIA</strong>, <strong>AMD</strong>, <strong>Intel</strong>, <strong>Google TPU</strong>, and <strong>AWS Trainium/Inferentia</strong></td><td>Supports <strong>NVIDIA</strong>, <strong>AMD</strong>, <strong>Intel</strong>, <strong>PowerPC</strong>, <strong>TPU</strong>, and <strong>AWS Trainium/Inferentia</strong></td></tr><tr><td><strong>Multi-LoRA Support</strong></td><td>Supports Multi-LoRA for adding multiple task-specific LoRA layers without retraining the model</td><td>Also supports Multi-LoRA for task-specific adaptation layers</td></tr><tr><td></td><td></td><td></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="unique-features">Unique features<a class="hash-link" aria-label="Direct link to Unique features" title="Direct link to Unique features" href="/blog/inference-engines#unique-features">​</a></h3>
<p>This is where the engines diverge a bit, and we can more actively select the functionality we need.</p>
<ul>
<li>
<p>When we want to reduce latency without compromising on the output&#x27;s accuracy, <strong><a href="https://arxiv.org/abs/2211.17192" target="_blank" rel="noopener noreferrer">speculative decoding</a></strong> becomes important. It does this by predicting the next tokens ahead of time, running speculative branches of the model in parallel and only finalizing the output once it’s clear which tokens are likely to be correct. This technique is especially useful in real-time applications like conversational agents, where every millisecond of delay matters.</p>
</li>
<li>
<p>Efficient <strong><a href="https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24" target="_blank" rel="noopener noreferrer">beam search</a></strong> is useful in cases where high-quality output is necessary, but we also need to maintain scalability and performance. In applications like <em>machine translation</em> or <em>summarization</em>, beam search improves text coherence, and vLLM’s efficient beam search allows us to get high-quality outputs without sacrificing throughput. This makes it ideal for large-scale, enterprise-level deployments.</p>
<p>However, in scenarios where you need speed and low latency, such as in live chatbots or voice assistants, beam search may introduce delays. In these cases, simpler decoding strategies like greedy search or sampling might be preferred to ensure faster response times.</p>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Digital_watermarking" target="_blank" rel="noopener noreferrer">Watermarking</a></strong> is important when organizations need to ensure the traceability of AI-generated content, particularly in environments where content verification or intellectual property protection is a priority. Use cases such as legal document generation, journalism, and content moderation benefit from watermarking, which allows enterprises to ensure transparency while maintaining trust in the system’s integrity.</p>
</li>
<li>
<p><strong><a href="https://docs.vllm.ai/en/latest/automatic_prefix_caching/apc.html" target="_blank" rel="noopener noreferrer">Prefix caching</a></strong> is important when dealing with repeated queries in chatbots, customer service automation, or document generation systems. In these cases, multiple requests often share similar prefixes or input sequences, and recalculating the same intermediate results for each query would waste computational resources. This can also be very helpful for <a href="https://www.clarifai.com/blog/what-is-rag-retrieval-augmented-generation" target="_blank" rel="noopener noreferrer">RAG</a> use cases, where a long document would be repeatedly used to answer questions.</p>
</li>
</ul>
<table><thead><tr><th><strong>Feature</strong></th><th><strong>TGI</strong></th><th><strong>vLLM</strong></th></tr></thead><tbody><tr><td><strong>Production Readiness</strong></td><td>Distributed tracing with OpenTelemetry and metrics tracking with Prometheus</td><td>No emphasis on production monitoring, more focused on performance and throughput</td></tr><tr><td><strong>Speculative Decoding</strong></td><td>Not available</td><td>Supports speculative decoding for faster response time by predicting token generation</td></tr><tr><td><strong>Guidance for Structured Output</strong></td><td>Supports Guidance, enabling function calling and structured outputs based on schemas</td><td>No equivalent feature, focuses on flexible decoding algorithms like beam search</td></tr><tr><td><strong>Watermarking</strong></td><td>Includes <strong><a href="https://arxiv.org/abs/2301.10226" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a></strong> to track model output</td><td>No watermarking feature available</td></tr><tr><td><strong>Prefix Caching</strong></td><td>Not available</td><td>Supports prefix caching to reduce latency for repeated queries</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="so-which-engine-is-more-performant">So which engine is more performant?<a class="hash-link" aria-label="Direct link to So which engine is more performant?" title="Direct link to So which engine is more performant?" href="/blog/inference-engines#so-which-engine-is-more-performant">​</a></h2>
<p>To begin choosing which engine to use, we can note that TGI is well-suited for enterprises looking for production-ready deployments with robust monitoring and low-latency features such as token streaming and guidance for structured outputs. Its strong focus on observability and ease of deployment makes it ideal for applications requiring real-time interaction.</p>
<p>vLLM, on the other hand, is designed for enterprises focused on high-throughput serving of large models, with advanced memory management, broad quantization support, and superior distributed inference capabilities. It is particularly effective in multi-node and multi-GPU environments that demand maximum scalability and efficiency.</p>
<p>That said, there are still other factors, because when we talk about the performance of an LLM inference engine, there are multiple considerations.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="latency-vs-throughput">Latency vs. Throughput<a class="hash-link" aria-label="Direct link to Latency vs. Throughput" title="Direct link to Latency vs. Throughput" href="/blog/inference-engines#latency-vs-throughput">​</a></h3>
<p>While these might sound like synonyms, they&#x27;re actually orthogonal measurements.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Orthogonal measurements?</div><div class="admonitionContent_BuS1"><p>When we say that latency and throughput are orthogonal measurements, it means they measure two independent aspects of system performance that don’t necessarily affect each other directly, and improving one does not automatically improve the other. However, system architecture often forces trade-offs between the two. When you optimize one, it can indirectly affect the other depending on how the system is designed.</p></div></div>
<ul>
<li><strong>Latency</strong> is the time it takes to process and return a response to a single request (how fast the system responds to one user).</li>
<li><strong>Throughput</strong> is the number of requests or tokens the system can process per second (how much work it can handle in total).</li>
</ul>
<p>While they are theoretically independent, in real-world systems, optimizing for one can affect the other. Minimizing latency for each request might reduce throughput because the system focuses on quickly responding to each user, possibly limiting how many users it can handle at once. Conversely, increasing throughput (handling more requests at once) might lead to higher latency per user because the system may batch requests or process them sequentially.</p>
<p>This video can help visualize this, showing that the LLM engine is producing 4 tokens per second; however, those are distributed among 4 users, so for each user it appears that it&#x27;s only producing 1 token per second.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/medias/LatencyThroughputVisualization-d3416c070c2f04dfca64adc07fe280b4.webm" data-pswp-width="0" data-pswp-height="0"><video src="/assets/medias/LatencyThroughputVisualization-d3416c070c2f04dfca64adc07fe280b4.webm" alt="A visual demonstration of the difference between latency and throughput" controls="" muted="" loop="" autoplay="" style="max-width:100%;height:auto"></video></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>A visual demonstration of the difference between latency and throughput in inference engines. Source: <a href="https://huggingface.co/blog/tgi-benchmarking" target="_blank" rel="noopener noreferrer">Hugging Face</a></span></figcaption></figure>
<p>There are even more metrics to consider (the source again is <a href="https://huggingface.co/blog/tgi-benchmarking" target="_blank" rel="noopener noreferrer">Hugging Face</a>)</p>
<table><thead><tr><th><strong>Term</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td><strong>Token Latency</strong></td><td>The amount of time it takes 1 token to be processed and sent to a user.</td></tr><tr><td><strong>Request Latency</strong></td><td>The amount of time it takes to fully respond to a request.</td></tr><tr><td><strong>Time to First Token</strong></td><td>The amount of time from the initial request to the first token returning to the user. This is a combination of the time to process the prefill input and a single generated token.</td></tr><tr><td><strong>Throughput</strong></td><td>The number of tokens the server can return in a set amount of time (e.g., 4 tokens per second).</td></tr></tbody></table>
<p>which leads to this useful table on what to emphasize:</p>
<table><thead><tr><th><strong>I care about…</strong></th><th><strong>I should focus on…</strong></th></tr></thead><tbody><tr><td><strong>Handling more users</strong></td><td>Maximizing Throughput</td></tr><tr><td><strong>People not navigating away from my page/app</strong></td><td>Minimizing Time to First Token (TTFT)</td></tr><tr><td><strong>User Experience for a moderate amount of users</strong></td><td>Minimizing Latency</td></tr><tr><td><strong>Well-rounded experience</strong></td><td>Capping latency and maximizing throughput</td></tr></tbody></table>
<p>Both <a href="https://huggingface.co/blog/tgi-benchmarking" target="_blank" rel="noopener noreferrer">TGI</a> and <a href="https://docs.vllm.ai/en/v0.5.4/performance_benchmark/benchmarks.html" target="_blank" rel="noopener noreferrer">vLLM</a> offer benchmarking utilities, so the best course of action is to determine your use case and priorities, and experiment with both.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="licensing">Licensing<a class="hash-link" aria-label="Direct link to Licensing" title="Direct link to Licensing" href="/blog/inference-engines#licensing">​</a></h2>
<p><strong>vLLM</strong> has always used the Apache 2.0 license, a permissive open-source license that allows users to freely use, modify, and distribute software, provided they include proper attribution, a copy of the license, and a notice of changes, while also offering protection from patent claims.</p>
<p><strong>TGI</strong> initially used the Apache 2.0 license, but then in <a href="https://www.reddit.com/r/MachineLearning/comments/15c89r7/d_huggingface_changed_the_license_of_one_of_its/" target="_blank" rel="noopener noreferrer">July 2023</a>, it changed its license from Apache 2.0 to the Hugging Face Optimized Inference License 1.0 (HFOILv1.0). This was widely disliked because it imposed the following restrictions on its previous Apache 2.0 license:</p>
<ol>
<li><strong>No Hosting Services:</strong> You can’t offer the software as a paid online service without getting permission from Hugging Face.</li>
<li><strong>No Changing the License:</strong> You can’t relicense the software under different rules.</li>
<li><strong>Patent Clause:</strong> If you sue someone over patent issues related to the software, you lose the right to use it.</li>
<li><strong>Modification Requirements:</strong> If you change the software, you have to clearly state what changes you made and include that information when sharing it.</li>
<li><strong>No Use of Trademarks:</strong> You can’t use Hugging Face’s logos or trademarks without their approval.</li>
</ol>
<p>In a video presentation from vLLM, you can <a href="https://youtu.be/5ZlavKF_98U?t=1760" target="_blank" rel="noopener noreferrer">hear the crowd&#x27;s reaction</a> when they point out that vLLM is still Apache 2.0 licensed.</p>
<p>Hugging Face later reverted back to Apache 2.0 in <a href="https://github.com/huggingface/text-generation-inference/commit/ff42d33e9944832a19171967d2edd6c292bdb2d6" target="_blank" rel="noopener noreferrer">April 2024</a> so at this point their licensing is equivalent to vLLM. Both are open source software.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="brief-code-samples">Brief code samples<a class="hash-link" aria-label="Direct link to Brief code samples" title="Direct link to Brief code samples" href="/blog/inference-engines#brief-code-samples">​</a></h2>
<p>For vLLM, the documentation is excellent and this code sample is taken directly from <a href="https://docs.vllm.ai/en/v0.5.4/getting_started/quickstart.html" target="_blank" rel="noopener noreferrer">their quickstart.</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vllm">vLLM<a class="hash-link" aria-label="Direct link to vLLM" title="Direct link to vLLM" href="/blog/inference-engines#vllm">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="installing-vllm">Installing vLLM<a class="hash-link" aria-label="Direct link to Installing vLLM" title="Direct link to Installing vLLM" href="/blog/inference-engines#installing-vllm">​</a></h4>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">Using pip</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># (Recommended) Create a new conda environment.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda create </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">n myenv python</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3.10</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">y</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate myenv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Install vLLM with CUDA 12.1.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install vllm</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="consuming-vllm">Consuming vLLM<a class="hash-link" aria-label="Direct link to Consuming vLLM" title="Direct link to Consuming vLLM" href="/blog/inference-engines#consuming-vllm">​</a></h4>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">Performing inference using Hugging Face</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> vllm </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LLM</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> SamplingParams</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Sample prompts.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">prompts </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;Hello, my name is&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;The president of the United States is&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;The capital of France is&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;The future of AI is&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Create a sampling params object.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sampling_params </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> SamplingParams</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">temperature</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.8</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> top_p</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.95</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Create an LLM.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llm </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> LLM</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;facebook/opt-125m&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Generate texts from the prompts. The output is a list of RequestOutput objects</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># that contain the prompt, generated text, and other information.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> llm</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">generate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">prompts</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> sampling_params</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Print the outputs.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> output </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> outputs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    prompt </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">prompt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    generated_text </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">outputs</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">text</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string-interpolation string" style="color:#e3116c">f&quot;Prompt: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">prompt</span><span class="token string-interpolation interpolation conversion-option punctuation" style="color:#393A34">!r</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, Generated text: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">generated_text</span><span class="token string-interpolation interpolation conversion-option punctuation" style="color:#393A34">!r</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tgi">TGI<a class="hash-link" aria-label="Direct link to TGI" title="Direct link to TGI" href="/blog/inference-engines#tgi">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="launching-tgi">Launching TGI<a class="hash-link" aria-label="Direct link to Launching TGI" title="Direct link to Launching TGI" href="/blog/inference-engines#launching-tgi">​</a></h4>
<p>TGI is distributed using Docker, so it&#x27;s a bit different to set up, but also straightforward. You need to first <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">install Docker</a> and then proceed. Like vLLM above, this is taken verbatim from the <a href="https://huggingface.co/docs/text-generation-inference/en/quicktour?code=python" target="_blank" rel="noopener noreferrer">TGI Quick Tour</a> page.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">Launching a Docker container with teknium/OpenHermes-2.5-Mistral-7B on an NVIDIA GPU</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">model=teknium/OpenHermes-2.5-Mistral-7B</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ghcr.io/huggingface/text-generation-inference:2.3.0 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --model-id $model```</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="consuming-tgi-python-example">Consuming TGI (Python example)<a class="hash-link" aria-label="Direct link to Consuming TGI (Python example)" title="Direct link to Consuming TGI (Python example)" href="/blog/inference-engines#consuming-tgi-python-example">​</a></h4>
<p>Once TGI is running, you can use the generate endpoint or the Open AI Chat Completion API compatible Messages API by doing requests.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">Consuming TGI with Python</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> requests</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">headers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;Content-Type&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;application/json&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">data </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&#x27;inputs&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;What is Deep Learning?&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&#x27;parameters&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token string" style="color:#e3116c">&#x27;max_new_tokens&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> requests</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">post</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;http://127.0.0.1:8080/generate&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> headers</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">headers</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> json</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">data</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">response</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">json</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># {&#x27;generated_text&#x27;: &#x27;\n\nDeep Learning is a subset of Machine Learning that is concerned with the development of algorithms that can&#x27;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="either-vllm-or-tgi-will-likely-suit-your-needs">Either vLLM or TGI will likely suit your needs<a class="hash-link" aria-label="Direct link to Either vLLM or TGI will likely suit your needs" title="Direct link to Either vLLM or TGI will likely suit your needs" href="/blog/inference-engines#either-vllm-or-tgi-will-likely-suit-your-needs">​</a></h2>
<p>The best inference engine ultimately depends on your specific use case. Each engine offers distinct optimizations for different enterprise needs. <strong>TGI</strong> is great for real-time, production-ready applications with strong monitoring tools and structured output guidance, while vLLM shines in high-throughput environments with advanced memory management and distributed inference capabilities. The best way to decide is by experimenting and benchmarking each engine using the model and hardware specific to your workload, allowing you to tailor the solution to your priorities, whether that’s <strong>low latency</strong>, <strong>high throughput</strong>, or <strong>scalability.</strong></p>
<p>And then do that every couple of months when everything changes again.</p>
<figure style="border:1px dashed rgba(0, 0, 0, .1);padding:0;margin:0;margin-bottom:20px;border-radius:15px;text-align:right" id="figure-gallery"><a href="/assets/images/shrug-a2ee30b27da5c96b3201ad1b30b37974.jpg" data-pswp-width="0" data-pswp-height="0"><img src="/assets/images/shrug-a2ee30b27da5c96b3201ad1b30b37974.jpg" alt="A racecar driver shrugging" style="max-width:100%;height:auto"></a><hr style="margin:5px 0;background-color:rgba(0, 0, 0, .2)"><figcaption style="margin-top:0.5em;margin-bottom:0.5em;margin-right:1em;text-align:right;font-size:0.8em"><span>See you in a month or two!</span><br><span>Generated with OpenAI DALL-E 3 and edited by the author.</span></figcaption></figure></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/chat-gpt">ChatGPT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">AI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/v-llm">vLLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/paged-attention">PagedAttention</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/local-ai">LocalAI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/open-weights-models">OpenWeightsModels</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/llama-models">LlamaModels</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/inference-engine">InferenceEngine</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/kv-cache">KVCache</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/secret-chickens-tuning"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Secret LLM chickens II: Tuning the chicken</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#what-is-an-llm-inference-engine">What is an LLM inference engine?</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#vllm-and-pagedattention">vLLM and PagedAttention</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#contiguous-memory-traditional-memory-allocation">Contiguous Memory (Traditional Memory Allocation)</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#pagedattention-paged-memory-allocation">PagedAttention (Paged Memory Allocation)</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#how-does-vllm-compare-to-tgi">How does vLLM compare to TGI?</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#commonalities">Commonalities</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#unique-features">Unique features</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#so-which-engine-is-more-performant">So which engine is more performant?</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#latency-vs-throughput">Latency vs. Throughput</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#licensing">Licensing</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#brief-code-samples">Brief code samples</a><ul><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#vllm">vLLM</a></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#tgi">TGI</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/blog/inference-engines#either-vllm-or-tgi-will-likely-suit-your-needs">Either vLLM or TGI will likely suit your needs</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/kelkulus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/iankelk" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Blog Feeds</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://kelk.ai/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">RSS<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://kelk.ai/blog/atom.xml" target="_blank" rel="noopener noreferrer" class="footer__link-item">Atom<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://kelk.ai/blog/feed.json" target="_blank" rel="noopener noreferrer" class="footer__link-item">JSON<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Short Attention Blog</div></div></div></footer></div>
</body>
</html>